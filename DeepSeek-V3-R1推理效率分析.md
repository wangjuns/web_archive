DeepSeek-V3/R1æ¨ç†æ•ˆç‡åˆ†æ
Original æ¸£B zartbot 2025å¹´03æœˆ14æ—¥ 11:00
æœ¬æ–‡ç”±äºè®¡ç®—é‡éå¸¸å¤§,æ­¥éª¤ä¹Ÿå¾ˆå¤šéš¾å…æœ‰é”™è¯¯ä¹‹å¤„,æ¬¢è¿å¤§å®¶æŒ‡æ­£.å¹¶ä¸”æ¯ä¸ªè®¡ç®—æ—¶ä½¿ç”¨çš„å‡½æ•°éƒ½å·²åˆ—å‡º,å¯ä¾›å¤§å®¶è‡ªè¡Œä¿®æ”¹. æœ¬æ–‡ä»…ä»£è¡¨ä¸ªäººè§‚ç‚¹, ä¸ä»»èŒçš„æœºæ„æ— å…³.

æ¯å½“çœ‹åˆ°ä»£ç é‡Œæœ‰Low Latencyçš„å­—çœ¼æ—¶, ç½‘å…šä»¬å°±æ¿€åŠ¨çš„ä¸è¡Œè¦å»é™ä½é™æ€å»¶è¿Ÿæ‰©å¤§å¸¦å®½, ä¸Šå¤§è§„æ¨¡ScaleUPç½‘ç»œ, ä½†æ˜¯äº‹å®ä¸Šæ˜¯å¦æ˜¯è¿™æ ·å‘¢? æœ¬æ–‡ä½œä¸ºç¬¬ä¸€ç¯‡åˆ†æä¸€ä¸‹H800å’ŒH20åœ¨DeepSeek-R1 EPå¹¶è¡Œçš„æ¨ç†æ€§èƒ½å³°å€¼, åç»­å°†ç»§ç»­åˆ†æB200-NVL72è¿™æ ·çš„å®ä¾‹, çœ‹çœ‹ScaleUPç½‘ç»œæ˜¯å¦æœ‰ä¼˜åŠ¿.

TL;DR

H800å’ŒH20åˆ†æç»“æœå¦‚ä¸‹æ‰€ç¤º, åŸºæœ¬ä¸ŠH800çš„æ•°æ®èƒ½å¤Ÿå’ŒDeepSeekå®˜æ–¹æ•°æ®å¯¹é½.

Prefillé˜¶æ®µ



	
H800
	
H20


TPS(Overlap)
	
52240.1
	
9377.0


TPS
	
33741.0
	
8536.9

Decodingé˜¶æ®µ



	
H800(TP1)
	
H800(TP1)
	
H800(TP1)
	
H20(TP4)
	
H20_3e(TP8)
	
H20_3e(TP8)


BatchSize
	
32.000
	
64.000
	
128.000
	
32.000
	
32.000
	
64.000


TPOT(Overlap)
	
9.858
	
19.716
	
39.431
	
35.367
	
29.613
	
49.005


TPOT
	
17.023
	
34.045
	
68.090
	
42.532
	
36.778
	
63.334


TPS(Overlap)
	
101.442
	
50.721
	
25.360
	
28.275
	
33.768
	
20.406


TPS
	
58.746
	
29.373
	
14.686
	
23.512
	
27.190
	
15.789


Total(Overlap)
	
3246.137
	
3246.137
	
3246.137
	
904.803
	
1080.591
	
1306.001


Total
	
1879.856
	
1879.856
	
1879.856
	
752.383
	
870.082
	
1010.516

å…¶ä¸­H20-3E,å³å¸¦HBM3e-141GBå†…å­˜çš„ç‰ˆæœ¬åœ¨Decodingé˜¶æ®µç›¸å¯¹äºH20çš„æ¥è¿‘1.4xçš„æ€§èƒ½æ”¶ç›Š.

æœ¬æ–‡ç›®å½•å¦‚ä¸‹:

1.Â DeepSeek-V3/R1æ¨¡å‹æ¶æ„åŠè®¡ç®—å¤æ‚åº¦åˆ†æ
1.1Â MLAè®¡ç®—å¤æ‚åº¦
1.1.1Â æ ‡å‡†æ¨¡å¼
1.1.2Â çŸ©é˜µå¸æ”¶æ¨¡å¼
1.2Â DenseMLPè®¡ç®—å¤æ‚åº¦
1.3Â MoE Expertè®¡ç®—å¤æ‚åº¦
1.4Â æ•°æ®æ±‡æ€»
2.Â Prefillé˜¶æ®µ
2.1Â MLAè®¡ç®—è€—æ—¶
2.2Â DenseMLPè®¡ç®—è€—æ—¶
2.3Â MoEè®¡ç®—è€—æ—¶
2.4Â AlltoAllé€šä¿¡è€—æ—¶
2.5Â æ€»è€—æ—¶
2.6Â Overlapåˆ†æ
2.7Â KVCacheè®¡ç®—
3.Â Decodingé˜¶æ®µ
3.1Â EPç­–ç•¥åˆ†æ
3.2Â Memoryåˆ©ç”¨ç‡åˆ†æ
3.3Â MLAè€—æ—¶
3.4Â DenseMLPè€—æ—¶
3.5Â AlltoAllé€šä¿¡è€—æ—¶
3.6Â æ€»è€—æ—¶
3.7Â Overlapåˆ†æ
4.Â å°ç»“

1. DeepSeek-V3/R1æ¨¡å‹æ¶æ„åŠè®¡ç®—å¤æ‚åº¦åˆ†æ

DeepSeek-V3/R1æ¨¡å‹æ¶æ„å¦‚ä¸‹

æ¨¡å‹çš„å‚æ•°å®šä¹‰å¦‚ä¸‹

classÂ ModelArgs:
Â  Â  max_batch_size: int =Â 8
Â  Â  max_seq_len: int =Â 4096Â *Â 4
Â  Â  vocab_size: int =Â 129280
Â  Â  dim: int =Â 7168
Â  Â  inter_dim: int =Â 18432
Â  Â  moe_inter_dim: int =Â 2048
Â  Â  n_layers: int =Â 61
Â  Â  n_dense_layers: int =Â 3
Â  Â  n_heads: int =Â 128
Â  Â Â # moe
Â  Â  n_routed_experts: int =Â 256
Â  Â  n_shared_experts: int =Â 1
Â  Â  n_activated_experts: int =Â 8
Â  Â  n_expert_groups: int =Â 8
Â  Â  n_limited_groups: int =Â 4
Â  Â  route_scale: float =Â 2.5
Â  Â Â # mla
Â  Â  q_lora_rank: int =Â 1536
Â  Â  kv_lora_rank: int =Â 512
Â  Â  qk_nope_head_dim: int =Â 128
Â  Â  qk_rope_head_dim: int =Â 64
Â  Â  v_head_dim: int =Â 128


è™½ç„¶å„ä¸ªæ¨¡å—çš„æµ®ç‚¹è¿ç®—é‡, å‚æ•°é‡è™½ç„¶ä¸€ç§å¾ˆç®€ä¾¿çš„åŠæ³•, ä½¿ç”¨ptflopsåº“ä¸­çš„get_model_complexity_infoç›´æ¥å¤„ç†blockå¾—å‡º, ä½†æ˜¯è¯¥åº“å¯¹äºä¸€äº›ç›¸å¯¹å¤æ‚çš„è¿ç®—è¿˜æ˜¯æœ‰ä¸€äº›é”™è¯¯, æœ¬æ–‡éƒ½è¿›è¡Œäº†æ‰‹å·¥æ ¡æ­£

1.1 MLAè®¡ç®—å¤æ‚åº¦
1.1.1 æ ‡å‡†å®ç°

MLAæ¨¡å—ä»£ç æ¥è‡ªDeepSeek-V3 Githubç¤ºä¾‹, å®ƒæ˜¯ä¸€ä¸ªæ ‡å‡†çš„MLAå®ç°

classÂ MLA(nn.Module):
Â  Â Â defÂ __init__(self, args: ModelArgs):
Â  Â  Â  Â  super().__init__()
Â  Â  Â  Â  self.dim = args.dimÂ #éšè—å±‚ç»´åº¦
Â  Â  Â  Â  self.n_heads = args.n_headsÂ 
Â  Â  Â  Â  self.n_local_heads = args.n_heads // world_size
Â  Â  Â  Â  self.q_lora_rank = args.q_lora_rankÂ #qçš„ä½ç§©å‹ç¼©çš„ç»´åº¦
Â  Â  Â  Â  self.kv_lora_rank = args.kv_lora_rankÂ #kvçš„ä½ç§©å‹ç¼©çš„ç»´åº¦
Â  Â  Â  Â  self.qk_nope_head_dim = args.qk_nope_head_dimÂ #qkä¸å¸¦æ—‹è½¬ä½ç½®ç¼–ç çš„å¤´çš„ç»´åº¦
Â  Â  Â  Â  self.qk_rope_head_dim = args.qk_rope_head_dimÂ #qkæ—‹è½¬ä½ç½®ç¼–ç çš„å¤´çš„ç»´åº¦
Â  Â  Â  Â  self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim
Â  Â  Â  Â  self.v_head_dim = args.v_head_dimÂ #vçš„å¤šå¤´æ³¨æ„åŠ›ä¸­å¤´çš„ç»´åº¦
Â  Â  Â  Â Â 
Â  Â  Â  Â  self.wq_a = nn.Linear(self.dim, self.q_lora_rank)
Â  Â  Â  Â Â #qçš„down-projectionçŸ©é˜µ
Â  Â  Â  Â Â 
Â  Â  Â  Â  self.q_norm = nn.RMSNorm(self.q_lora_rank)
Â  Â  Â  Â Â 
Â  Â  Â  Â  self.wq_b = nn.Linear(self.q_lora_rank, self.n_heads * self.qk_head_dim)
Â  Â  Â  Â Â #qçš„up-projectionçŸ©é˜µ
Â  Â  Â  Â Â 
Â  Â  Â  Â  self.wkv_a = nn.Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim)
Â  Â  Â  Â Â # wkv_aä¸ºKå’ŒVçš„down-projectionçŸ©é˜µ
Â  Â  Â  Â  self.kv_norm = nn.RMSNorm(self.kv_lora_rank)
Â  Â  Â  Â Â 
Â  Â  Â  Â  self.wkv_b = nn.Linear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim))
Â  Â  Â  Â Â # wkv_bä¸ºKå’ŒVçš„up-projectionçŸ©é˜µ
Â  Â  Â  Â Â 
Â  Â  Â  Â  self.wo = nn.Linear(self.n_heads * self.v_head_dim, self.dim)Â #outputæƒé‡çŸ©é˜µ
Â  Â  Â  Â  self.softmax_scale = self.qk_head_dim **Â -0.5Â #è®¡ç®—1/sqrt(d_k)
Â  Â  Â  Â  self.register_buffer("kv_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False)
Â  Â  Â  Â  self.register_buffer("pe_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim), persistent=False)
Â  Â  Â  Â Â 
Â  Â Â defÂ forward(self, x: torch.Tensor):
Â  Â  Â  Â  bsz, seqlen, _ = x.size()
Â  Â  Â  Â  start_pos =Â 1
Â  Â  Â  Â  end_pos = start_pos + seqlen
Â  Â  Â  Â Â # ---- è®¡ç®—q--------
Â  Â  Â  Â  q = self.wq_b(self.q_norm(self.wq_a(x)))
Â  Â  Â  Â  q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim)
Â  Â  Â  Â  q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)Â #åˆ†ç¦»nope,rope
Â  Â  Â  Â  q_pe = apply_rotary_emb(q_pe, freqs_cis)Â #æ‰§è¡ŒRoPEè®¡ç®—
Â  Â  Â  Â Â 
Â  Â  Â  Â Â # ----è®¡ç®—KV----------
Â  Â  Â  Â  kv = self.wkv_a(x)
Â  Â  Â  Â Â #KV-Cacheå¤§å°ä¸ºwkv_a outputdim(self.kv_lora_rank + self.qk_rope_head_dim)
Â  Â  Â  Â  kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)Â #åˆ†ç¦»KVå’ŒKä½ç½®ç¼–ç 
Â  Â  Â  Â  k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)Â #æ‰§è¡ŒRoPEè®¡ç®—
Â  Â  Â  Â Â 
Â  Â  Â  Â Â # -----å¤„ç†KV u-pprojectionçŸ©é˜µ
Â  Â  Â  Â  wkv_b = self.wkv_b.weightÂ 
Â  Â  Â  Â  wkv_b = wkv_b.view(self.n_local_heads,Â -1, self.kv_lora_rank)
Â  Â  Â  Â Â 
Â  Â  Â  Â Â # qä¸­ä¸éœ€è¦ä½ç½®ç¼–ç çš„å…ˆå’ŒKçš„ä¸éœ€è¦ä½ç½®ç¼–ç çš„æƒé‡ç›¸ä¹˜
Â  Â  Â  Â  q_nope = torch.einsum("bshd,hdc->bshc", q_nope, wkv_b[:, :self.qk_nope_head_dim])
Â  Â  Â  Â  self.kv_cache[:bsz, start_pos:end_pos] = self.kv_norm(kv)#ä¿å­˜KV Cache
Â  Â  Â  Â  self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2)Â #ä¿å­˜Kçš„ä½ç½®ç¼–ç Cache(pe cache)
Â  Â  Â  Â Â 
Â  Â  Â  Â Â # è®¡ç®—QK^T/sqrt(d_k)
Â  Â  Â  Â  scores = (torch.einsum("bshc,btc->bsht", q_nope, self.kv_cache[:bsz, :end_pos]) +
Â  Â  Â  Â  Â  Â  Â  Â  Â  torch.einsum("bshr,btr->bsht", q_pe, self.pe_cache[:bsz, :end_pos])) * self.softmax_scale
Â  Â  Â  Â  scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)
Â  Â  Â  Â Â 
Â  Â  Â  Â Â # è®¡ç®—V
Â  Â  Â  Â  x = torch.einsum("bsht,btc->bshc", scores, self.kv_cache[:bsz, :end_pos])
Â  Â  Â  Â  x = torch.einsum("bshc,hdc->bshd", x, wkv_b[:, -self.v_head_dim:])
Â  Â  Â  Â Â 
Â  Â  Â  Â  x = self.wo(x.flatten(2))Â #woæƒé‡, ä»n_head * v_head_dim -> dim
Â  Â  Â  Â Â returnÂ x


ä¸ºäº†ä¾¿äºç†è§£å…·ä½“çš„è®¡ç®—æµç¨‹, æˆ‘ä»¬å°†ä»£ç æ‰§è¡Œæµç¨‹å›¾å¦‚ä¸‹å›¾æ‰€ç¤º:

ä»å›¾ä¸Šå¯çŸ¥, å•ä¸ªTokençš„KVCacheç”¨é‡ä»forwardå‡½æ•°ä¸­çš„kv = self.wkv_a(x)ä¸­å¾—çŸ¥, ç»´åº¦ä¸ºkv_lora_rank(512)+ qk_rope_head_dim(64) ä¸º 576.

åˆ†æè®¡ç®—å¤æ‚åº¦å¦‚ä¸‹:

args = ModelArgs()
m = MLA(args)
num_tokens =Â 1

mla_flops, mla_params = get_model_complexity_info(m, (num_tokens,args.dim),as_strings=True,print_per_layer_stat=True)

##è¾“å‡ºç»“æœå¦‚ä¸‹
MLA(
Â Â 187.17Â M,Â 99.999% Params,Â 170.36Â MMac,Â 100.000% MACs,Â 
Â  (wq_a): Linear(11.01Â M,Â 5.883% Params,Â 11.01Â MMac,Â 6.464% MACs, in_features=7168, out_features=1536, bias=True)
Â  (q_norm): RMSNorm(0,Â 0.000% Params,Â 0.0Â Mac,Â 0.000% MACs, (1536,), eps=None, elementwise_affine=True)
Â  (wq_b): Linear(37.77Â M,Â 20.181% Params,Â 37.77Â MMac,Â 22.172% MACs, in_features=1536, out_features=24576, bias=True)
Â  (wkv_a): Linear(4.13Â M,Â 2.206% Params,Â 4.13Â MMac,Â 2.424% MACs, in_features=7168, out_features=576, bias=True)
Â  (kv_norm): RMSNorm(0,Â 0.000% Params,Â 0.0Â Mac,Â 0.000% MACs, (512,), eps=None, elementwise_affine=True)
Â  (wkv_b): Linear(16.81Â M,Â 8.981% Params,Â 0.0Â Mac,Â 0.000% MACs, in_features=512, out_features=32768, bias=True)
Â  (wo): Linear(117.45Â M,Â 62.748% Params,Â 117.45Â MMac,Â 68.940% MACs, in_features=16384, out_features=7168, bias=True)
)


å³å•ä¸ªMLA blockæœ‰187.17Mä¸ªå‚æ•°, å‚æ•°æ•°é‡æ²¡å•¥é—®é¢˜.

ä½†æ˜¯å•ä¸ªTokençš„è®¡ç®—å¤æ‚åº¦ä¸º 170.36M Macè¿™ä¸ªå€¼å®é™…ä¸Šæ˜¯æœ‰é”™è¯¯çš„, wkv_bç”±äºsplitä¸ºw_ukå’Œw_uv, ç®—åŠ›æ¶ˆè€—æ²¡æœ‰è®¡ç®—, å› æ­¤æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ‰‹å·¥è®¡ç®—çš„å‡½æ•°

defÂ mla_flops(q_len, kv_len, args:ModelArgs, kv_cache_rate=0):
Â  Â Â #calculate MACs and estimate Flops approx. 2xMAC.
Â  Â  q_down_proj = q_len * args.dim * args.q_lora_rankÂ #wq_a
Â  Â  q_up_proj = q_len * args.q_lora_rank * args.n_heads * (args.qk_nope_head_dim + args.qk_rope_head_dim)Â #wq_b
Â  Â  kv_down_proj = kv_len * args.dim * (args.kv_lora_rank + args.qk_rope_head_dim)Â #wkv_a
Â  Â  k_up_proj = kv_len * args.kv_lora_rank * args.n_heads * args.qk_nope_head_dimÂ #w_uk
Â  Â  v_up_proj = kv_len * args.kv_lora_rank * args.n_heads * args.v_head_dimÂ #w_uv

Â  Â  kv_down_proj = kv_down_proj * (1Â - kv_cache_rate)
Â  Â  gemm_sum = q_down_proj + q_up_proj + kv_down_proj + k_up_proj + v_up_proj
Â  Â Â 
Â  Â Â #æŠŠå®ƒçœ‹æˆä¸€ä¸ªæ ‡å‡†çš„args.n_headsçš„MHA
Â  Â  mha = args.n_heads * ( q_len * args.qk_rope_head_dim * kv_lenÂ #QK_score_rope
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  + q_len * args.qk_nope_head_dim * kv_lenÂ #QK_score_nope
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  + q_len * kv_len * args.v_head_dim)Â #ScoreV
Â  Â  wo = q_len * args.n_heads * args.v_head_dim * args.dimÂ #wo
Â  Â  attn_sum = mha + wo
Â  Â Â 
Â  Â Â #return flops by 2* Sum(MACs)
Â  Â  GEMM_FP8_FLOPS = gemm_sum *Â 2/1e9
Â  Â  ATTN_FP16_FLOPS = Â attn_sum *Â 2/1e9
Â  Â Â 
Â  Â Â returnÂ GEMM_FP8_FLOPS+ATTN_FP16_FLOPS, GEMM_FP8_FLOPS,ATTN_FP16_FLOPS


å•ä¸ªtokençš„å®é™…è¿ç®—å¤æ‚åº¦ä¸º:

mla_flops(1,1,args,0)

(0.37429248000000004,Â 0.139329536,Â 0.234962944)

1.1.2 çŸ©é˜µå¸æ”¶æ¨¡å¼

è¿™é‡Œè¿˜éœ€è¦å†æä¸€ç‚¹, åœ¨DeepSeek-V2çš„è®ºæ–‡ä¸­æåˆ°

Fortunately, due to the associative law of matrix multiplication, we can absorb ğ‘Š_ğ‘ˆğ¾ into ğ‘Šğ‘ˆğ‘„, and ğ‘Š_ğ‘ˆğ‘‰ into ğ‘Šğ‘‚

WU_Qå…¶å®å°±æ˜¯ä¸Šæ–‡ä»£ç ä¸­çš„wq_b. åœ¨ä¸Šå›¾ä¸­ç¬¬(3)æ­¥å‰å¯ä»¥å°†w_ukå…ˆå’Œwq_bç›¸ä¹˜. ä»¥åŠåœ¨ç¬¬(7)æ­¥ä¸­å¯ä»¥w_uvå’Œwoç›¸ä¹˜.å¦‚ä¸‹å›¾æ‰€ç¤º:

wq_b_nopeä¸º[q_lora_rank(1536),Â n_head(128) xÂ qk_nope_head_dim(128)]çŸ©é˜µ
w_ukä¸º[kv_lora_rank(512) ,Â n_head(128) xÂ qk_nope_head_dim(128)]çŸ©é˜µ

çŸ©é˜µå¸æ”¶ä»¥åçš„q_absorbä¸º['q_lora_rank'(1536),h_head(128)xÂ kv_lora_rank(512)].

åŒç†å¯¹woå¸æ”¶wu_våˆ†æå¦‚ä¸‹:

woä¸º[n_head(128) xÂ v_head_dim(128),Â dim(7168)]çŸ©é˜µ
w_uvä¸º[kv_lora_rank(512) ,Â n_head(128) xÂ v_head_dim(128)]çŸ©é˜µ

çŸ©é˜µå¸æ”¶ä»¥åçš„o_absorbä¸º[dim(7168),h_head(128)xÂ kv_lora_rank(512)].

å¯¹äºç®—åŠ›æ¶ˆè€—å®šä¹‰ä¸€ä¸ªå‡½æ•°å¦‚ä¸‹:

defÂ mla_matabsob_flops(q_len, kv_len, args:ModelArgs, kv_cache_rate=0):
Â  Â Â #calculate MACs and estimate Flops approx. 2xMAC.
Â  Â  q_down_proj = q_len * args.dim * args.q_lora_rankÂ #wq_a
Â  Â  q_rope_up_proj = q_len * args.q_lora_rank * args.n_heads * args.qk_rope_head_dimÂ #wq_b_rope
Â  Â  q_absorb = q_len * args.n_heads * args.q_lora_rank * args.kv_lora_rankÂ 
Â  Â Â 
Â  Â  kv_down_proj = kv_len * args.dim * (args.kv_lora_rank + args.qk_rope_head_dim)Â #wkv_a
Â  Â  kv_down_proj = kv_down_proj * (1Â - kv_cache_rate)Â #KV-Cacheå‘½ä¸­ç‡ä¿®æ­£
Â  Â  gemm_sum = q_down_proj + q_rope_up_proj + q_absorb + kv_down_projÂ 
Â  Â Â 
Â  Â Â #æŠŠå®ƒçœ‹æˆä¸€ä¸ªæ ‡å‡†çš„args.n_headsçš„MQA
Â  Â  mqa = args.n_heads * ( q_len * args.qk_rope_head_dim * kv_lenÂ #Score_rope
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  + q_len * args.kv_lora_rank * kv_lenÂ #Score_nope
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  + q_len * kv_len * args.kv_lora_rank)Â #Score V
Â  Â  o_absorb = q_len * args.n_heads * args.kv_lora_rank * args.dimÂ 
Â  Â  attn_sum = mqa + o_absorb
Â  Â Â 
Â  Â Â #return flops by 2* Sum(MACs)
Â  Â  gemm_sum = Â gemm_sum *Â 2/1e9
Â  Â  attn_sum = attn_sum *Â 2/1e9
Â  Â Â 
Â  Â Â returnÂ gemm_sum + attn_sum, gemm_sum,attn_sum


å¯¹äºå•ä¸ªTokençš„å®é™…è¿ç®—å¤æ‚åº¦ä¸º:

mla_matabsob_flops(1,1,args,0)

(1.196572672,Â 0.256770048,Â 0.939802624)



ç›¸å¯¹äºéå¸æ”¶çš„å¤æ‚åº¦ä¸ºmla_matabsob_flops(1,1,args,0)[0] / mla_flops(1,1,args,0)[0], è¿ç®—å¤æ‚åº¦åè€Œå¢åŠ äº†3.197å€.

å¯¹äºå¸æ”¶åçš„æ¨¡å‹å‚æ•°ä¼°è®¡å¦‚ä¸‹:

defÂ mla_matabsob_mem(args:ModelArgs):
Â  Â  q_down_proj = args.dim * args.q_lora_rankÂ #wq_a
Â  Â  q_rope_up_proj = Â args.q_lora_rank * args.n_heads * args.qk_rope_head_dimÂ #wq_b_rope
Â  Â  q_absorb = args.n_heads * args.q_lora_rank * args.kv_lora_rankÂ 
Â  Â  kv_down_proj = Â args.dim * (args.kv_lora_rank + args.qk_rope_head_dim)Â #wkv_a
Â  Â  o_absorb = args.n_heads * args.kv_lora_rank * args.dimÂ 
Â  Â Â returnÂ q_down_proj + q_rope_up_proj + q_absorb + kv_down_proj + o_absorb

mla_matabsob_mem(args)/1e6
598.147072


å‚æ•°æ•°é‡ä¸º598.14M, å‚æ•°è§„æ¨¡ä¹Ÿå¢åŠ äº†3.197å€.

ä½†æ˜¯, MLA_Absorbåœ¨Decodingé˜¶æ®µä¼šæœ‰é¢å¤–çš„æ”¶ç›Š, æœ‰å®˜æ–¹çš„æ•°æ®ã€ŠDeepSeek-V3 / R1 æ¨ç†ç³»ç»Ÿæ¦‚è§ˆã€‹[1],å¹³å‡æ¯è¾“å‡ºä¸€ä¸ª token çš„ KVCache é•¿åº¦æ˜¯4989, ä»¥æ­¤è®¡ç®—ä¸¤è€…æœ‰ç€æ˜¾è‘—çš„å·®å¼‚.

#Prefill
mla_matabsob_flops(4989,4989,args,0)[0] / mla_flops(4989,4989,args,0)[0]

3.3028

#Decodingæ—¶qlen=1,KVcacheä¸éœ€è¦è®¡ç®—kv_cache_rate=1
mla_matabsob_flops(1,4989,args,1)[0] / mla_flops(1,4989,args,1)[0]

0.015


ç»“è®º: åœ¨Prefillé˜¶æ®µé‡‡ç”¨éå¸æ”¶çš„ç‰ˆæœ¬, åœ¨Decodingé‡‡ç”¨çŸ©é˜µå¸æ”¶çš„ç‰ˆæœ¬.

1.2 DenseMLPè®¡ç®—å¤æ‚åº¦

åœ¨æ¨¡å‹çš„å‰ä¸‰å±‚é‡‡ç”¨Dense MLP, å…¶è®¡ç®—å¤æ‚åº¦å¦‚ä¸‹

classÂ DenseMLP(nn.Module):
Â  Â Â defÂ __init__(self, dim: int, inter_dim: int):
Â  Â  Â  Â  super().__init__()
Â  Â  Â  Â  self.w1 = nn.Linear(dim, inter_dim, dtype=torch.bfloat16)
Â  Â  Â  Â  self.w2 = nn.Linear(inter_dim, dim, dtype=torch.bfloat16)
Â  Â  Â  Â  self.w3 = nn.Linear(dim, inter_dim, dtype=torch.bfloat16)

Â  Â Â defÂ forward(self, x: torch.Tensor)Â -> torch.Tensor:
Â  Â  Â  Â Â returnÂ self.w2(F.silu(self.w1(x)) * self.w3(x))

args = ModelArgs()
#dim=7168,inter_dim=18432
d = DenseMLP(args.dim, args.inter_dim)
num_tokens =Â 1

mlp_flops, mlp_params = get_model_complexity_info(d, (1,num_tokens,args.dim),as_strings=True,print_per_layer_stat=True)
##è¾“å‡ºç»“æœå¦‚ä¸‹:
DenseMLP(
Â Â 396.41Â M,Â 100.000% Params,Â 396.41Â MMac,Â 99.995% MACs,Â 
Â  (w1): Linear(132.14Â M,Â 33.334% Params,Â 132.14Â MMac,Â 33.333% MACs, in_features=7168, out_features=18432, bias=True)
Â  (w2): Linear(132.13Â M,Â 33.331% Params,Â 132.13Â MMac,Â 33.330% MACs, in_features=18432, out_features=7168, bias=True)
Â  (w3): Linear(132.14Â M,Â 33.334% Params,Â 132.14Â MMac,Â 33.333% MACs, in_features=7168, out_features=18432, bias=True)
)


å•ä¸ªMLP blockæœ‰396.41Mä¸ªå‚æ•°, å•ä¸ªTokençš„è®¡ç®—å¤æ‚åº¦ä¸º 396.41M Mac~792.82MFLOPS.å®šä¹‰DenseMLPè®¡ç®—å¤æ‚åº¦å‡½æ•°å¦‚ä¸‹:

defÂ densmlp_flops(args:ModelArgs, seq_len):
Â  Â Â returnÂ 3Â * seq_len * args.dim * args.inter_dim *2Â /1e9

1.3 MoE Expertè®¡ç®—å¤æ‚åº¦

åœ¨æ¨¡å‹çš„å58å±‚é‡‡ç”¨äº†MoE, Â å…¶è®¡ç®—å¤æ‚åº¦å¦‚ä¸‹

classÂ Expert(nn.Module):
Â  Â Â defÂ __init__(self, dim: int, inter_dim: int):
Â  Â  Â  Â  super().__init__()
Â  Â  Â  Â  self.w1 = nn.Linear(dim, inter_dim, dtype=torch.bfloat16)
Â  Â  Â  Â  self.w2 = nn.Linear(inter_dim, dim, dtype=torch.bfloat16)
Â  Â  Â  Â  self.w3 = nn.Linear(dim, inter_dim, dtype=torch.bfloat16)

Â  Â Â defÂ forward(self, x: torch.Tensor)Â -> torch.Tensor:
Â  Â  Â  Â Â returnÂ self.w2(F.silu(self.w1(x)) * self.w3(x))
args = ModelArgs()
num_tokens =Â 1

#dim=7168,moe_inter_dim=2048
e = Expert(args.dim, args.moe_inter_dim) Â  Â  Â  Â 

moe_flops, moe_params = get_model_complexity_info(e, (1,num_tokens,args.dim),as_strings=True,print_per_layer_stat=True)
##è¾“å‡ºç»“æœå¦‚ä¸‹:
Expert(
Â Â 44.05Â M,Â 100.000% Params,Â 44.05Â MMac,Â 99.995% MACs,Â 
Â  (w1): Linear(14.68Â M,Â 33.329% Params,Â 14.68Â MMac,Â 33.328% MACs, in_features=7168, out_features=2048, bias=True)
Â  (w2): Linear(14.69Â M,Â 33.341% Params,Â 14.69Â MMac,Â 33.340% MACs, in_features=2048, out_features=7168, bias=True)
Â  (w3): Linear(14.68Â M,Â 33.329% Params,Â 14.68Â MMac,Â 33.328% MACs, in_features=7168, out_features=2048, bias=True)
)


å•ä¸ªMoE Expertæœ‰44.05Mä¸ªå‚æ•°, å•ä¸ªTokençš„è®¡ç®—å¤æ‚åº¦ä¸º 44.05M Mac~88.1MFLOPS, å®šä¹‰MoE Expertè®¡ç®—å¤æ‚åº¦å‡½æ•°å¦‚ä¸‹:

defÂ moe_expert_flops(args:ModelArgs, seq_len):
Â  Â Â returnÂ 3Â * seq_len * args.dim * args.moe_inter_dim *2/1e9

1.4 æ•°æ®æ±‡æ€»

æ¨¡å‹æ•´ä½“çš„å‚æ•°åˆ†å¸ƒå¦‚è¡¨æ‰€ç¤º, å¦å¤–æ¨¡å‹è¿˜æœ‰MoE Gatingå‡½æ•°, å‚æ•°é‡ä¸ºdim x n_routed_expert + n_routed_expert(bias)=1.83M, ä»¥åŠEmbeddingå’ŒOutputå±‚çš„å‚æ•°vocab_size x dim=926.67M,

Block
	
å•å±‚å‚æ•°é‡
	
å±‚æ•°
	
ç´¯è®¡å‚æ•°


MLA
	
187.17M
	
61
	
11.41B


DenseMLP
	
396.41M
	
3
	
1.19B


Expert
	
44.05Mx(256_routed+1_shared)
	
58
	
656.6B


Gate
	
1.83M
	
58
	
106.14M


Embedding
	
926.67M
	
1
	
926.67M


Output
	
926.67M
	
1
	
926.67M


SUM
	
-
	
-
	
671.16B

å¯¹äºä¸åŒblockçš„ç®—åŠ›æ¶ˆè€—ç»Ÿè®¡å¦‚ä¸‹

Block
	
å‚æ•°é‡
	
è¿ç®—å¤æ‚åº¦(FLops)
	
KVCacheç”¨é‡


MLA
	
187.17M
	
374.29M
	
576B(FP8)


MLA_absorb
	
598.14M
	
1196.57M
	
576(FP8)


DenseMLP
	
396.41M
	
792.82M
	
-


Expert
	
44.05 M
	
488.1M
	
-

å®é™…è®¡ç®—æ—¶ä¼šæŒ‰ç…§å…·ä½“çš„Prefillå’ŒDecodeä»¥åŠKVCacheå‘½ä¸­ç‡è¿›è¡Œè¯„ä¼°.

KVCacheç”¨é‡Â :å•ä¸ªToken çš„KVCacheéœ€è¦ç´¯ç§¯61å±‚,å®é™…æ¶ˆè€—æŒ‰ç…§FP16ä¿å­˜KVCacheä¸º 2x 576x 61 =68.62KB. æŒ‰ç…§FP8ä¿å­˜ä¸º34.31KB.

H20/H800ç®—åŠ›æŒ‡æ ‡å¦‚ä¸‹è¡¨æ‰€ç¤º

GPUç±»å‹
	
SM
	
FP16ç®—åŠ›
	
FP8ç®—åŠ›
	
æ˜¾å­˜å¤§å°
	
æ˜¾å­˜å¸¦å®½
	
NVLINKå¸¦å®½
	
PCIeå¸¦å®½


H800
	
132
	
989.5
	
1979
	
80GB
	
3350
	
200
	
50


H20
	
78
	
148
	
296
	
96GB
	
3350
	
450
	
50
æ³¨: ç®—åŠ›å•ä½ä¸ºTFLOPS,å¸¦å®½å•ä½ä¸ºGB/s

ä¸ºäº†ä¾¿äºåç»­è®¡ç®—, å®šä¹‰GPUæ€§èƒ½å‡½æ•°å¦‚ä¸‹æ‰€ç¤º, GPUæ€§èƒ½ä¼°è®¡æŒ‰ç…§å³°å€¼çš„85%ä¼°è®¡. H800éœ€è¦24ä¸ªé€šä¿¡SM. è¿™é‡Œè€ƒè™‘åˆ°H20æµ®ç‚¹ç®—åŠ›æ¯”è¾ƒå¼±, H20ä¼°è®¡éœ€è¦10ä¸ªé€šä¿¡SM,

classÂ GPU_perf():
Â  Â Â defÂ __init__(self,sm,comm_sm, fp16_flops,fp8_flops,mem,mem_bw, nvlink_bw,pcie_bw, discount_rate):
Â  Â  Â  Â  self.sm = sm
Â  Â  Â  Â  self.comm_sm = comm_smÂ #ç”¨äºé€šä¿¡çš„SMæ•°é‡
Â  Â  Â  Â  self.fp16_flops = fp16_flops
Â  Â  Â  Â  self.fp8_flops = fp8_flops
Â  Â  Â  Â  self.mem = mem
Â  Â  Â  Â  self.mem_bw = mem_bw
Â  Â  Â  Â  self.nvlink_bw = nvlink_bw
Â  Â  Â  Â  self.pcie_bw = pcie_bw
Â  Â  Â  Â  self.discount_rate = discount_rateÂ #æ•´ä½“æ€§èƒ½æŒ‰å³°å€¼æ€§èƒ½æŠ˜æ‰£
Â  Â  Â  Â Â #TODO:Â å¯ä»¥åˆ†ç¦»ç½‘ç»œæ€§èƒ½æŠ˜æ‰£å’Œç®—åŠ›æ€§èƒ½æŠ˜æ‰£

Â  Â Â defÂ get_fp16_flops(self):
Â  Â  Â  Â Â returnÂ self.fp16_flops * self.discount_rate Â * ( self.sm Â - self.comm_sm) / self.sm

Â  Â Â defÂ get_fp8_flops(self):
Â  Â  Â  Â Â returnÂ self.fp8_flops * Â self.discount_rate * ( self.sm Â - self.comm_sm) / self.sm

Â  Â Â defÂ get_mem_bw(self):
Â  Â  Â  Â Â returnÂ self.mem_bw * Â self.discount_rate

Â  Â Â defÂ get_nvlink_bw(self):
Â  Â  Â  Â Â returnÂ self.nvlink_bw * Â self.discount_rate

Â  Â Â defÂ get_pcie_bw(self):
Â  Â  Â  Â Â returnÂ self.pcie_bw * Â self.discount_rate

h800 = GPU_perf( sm =Â 132Â ,comm_sm =Â 24,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 791.6, fp8_flops =Â 1583.2,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 80,mem_bw =Â 3350,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 200,pcie_bw =Â 50,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)

h20 = GPU_perf( sm =Â 78Â ,comm_sm =Â 10,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 96,mem_bw =Â 3350,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 50,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)

gpu = dict({'H800': h800,Â 'H20': h20})

2. Prefillé˜¶æ®µ

ä»DeepSeekå®˜æ–¹çš„æŠ¥å‘Šå¯çŸ¥, Prefillï¼šè·¯ç”±ä¸“å®¶ EP32ã€MLA å’Œå…±äº«ä¸“å®¶ DP32ï¼Œä¸€ä¸ªéƒ¨ç½²å•å…ƒæ˜¯ 4 èŠ‚ç‚¹ï¼Œ32 ä¸ªå†—ä½™è·¯ç”±ä¸“å®¶ï¼Œæ¯å¼ å¡ 9 ä¸ªè·¯ç”±ä¸“å®¶å’Œ 1 ä¸ªå…±äº«ä¸“å®¶. å¦ä¸€æ–¹é¢Attentionè®¡ç®—å¹¶è¡Œç­–ç•¥å‚è€ƒè®ºæ–‡ä¸­çš„æè¿°

The minimum deployment unit of prefilling stage consists of 4 nodes with 32 GPUs. The attention part employs 4-way Tensor Parallelism (TP4) with Sequence Parallelism (SP), combined with 8-way Data Parallelism (DP8). For the MoE part, we use 32-way Expert Parallelism (EP32)

ä»¥Attentionçš„è§†è§’æ¥çœ‹, æ¨ç†è¯·æ±‚åœ¨API Serveré€šè¿‡è´Ÿè½½å‡è¡¡å™¨ä»¥DP=8åˆ†é…åˆ°ä¸åŒçš„PrefillèŠ‚ç‚¹çš„DPç»„å†…, ä¸€ä¸ªDPç»„å†…æœ‰4å¼ H800æ„æˆä¸€ä¸ªTP+SPçš„ç»„è¿›è¡ŒMLAè®¡ç®—. ç„¶åä»¥MoEçš„è§†è§’æ¥çœ‹, 32ä¸ªGPUç»„æˆä¸€ä¸ªEP32çš„Group, æ¯ä¸€å±‚256ä¸ªExpertå¹³å‡æ¯å¼ å¡8ä¸ªRouted Expert,ç„¶åæ¯ä¸€å¼ å¡è¿˜æœ‰ä¸€ä¸ªShared Expert, å¹¶æ ¹æ®è®ºæ–‡å†æ‰¿è½½ä¸€ä¸ªRedundant Expert, ç´¯è®¡10ä¸ªExpert.

2.1 MLAè®¡ç®—è€—æ—¶

æŒ‰ç…§æ–‡ç« ã€ŠDeepSeek V3/R1 æ¨ç†æ•ˆç‡åˆ†æï¼ˆ2ï¼‰: DeepSeek æ»¡è¡€ç‰ˆé€†å‘å·¥ç¨‹åˆ†æã€‹[2]ä¸­æåˆ°çš„çŸ¥ä¹ä½œè€…@å¤©é˜¿è¥¿å§æåˆ°çš„Prefillå’ŒDecodingé•¿åº¦åˆ†æ:

å‡è®¾Pä»£è¡¨sequenceçš„å¹³å‡è¾“å…¥é•¿åº¦ï¼ŒDä»£è¡¨sequenceçš„å¹³å‡è¾“å‡ºé•¿åº¦ï¼Œé‚£å¯¹äºæ¯ä¸€ä¸ªè¾“å‡ºtokençš„å¹³å‡KVcacheçš„é•¿åº¦çº¦ç­‰äºP+D/2=4989; å†åŠ ä¸ŠP/D=608B/168Bï¼›Pçš„å–å€¼å¤§æ¦‚ä¸º4383ï¼ŒDçš„å–å€¼å¤§æ¦‚ä¸º1210

ä»¥å¹³å‡PrefillÂ seq_lenä¸º4383è®¡ç®—, KVCacheå‘½ä¸­ç‡æŒ‰ç…§å®˜æ–¹çš„ 56.3%è®¡ç®—, GPUæ€§èƒ½ä¼°è®¡æŒ‰ç…§å³°å€¼çš„85%ä¼°è®¡.

å®šä¹‰è®¡ç®—å‡½æ•°å¦‚ä¸‹, å¹¶è€ƒè™‘TPå¹¶è¡Œçš„æƒ…å†µ, æˆ‘ä»¬å‡è®¾åœ¨seq_lenä¸­æœ‰56.3%çš„é•¿åº¦æ˜¯å¯ä»¥ä»KVCacheä¸­æå–çš„, é‚£ä¹ˆPrefillçš„æ—¶å€™å°±éœ€è¦è®¡ç®—(1-kv_cache_rate)çš„token.

defÂ prefill_mla_elapse_time(args:ModelArgs,gpu:GPU_perf, discount, comm_sm, seq_len, kv_cache_rate):
Â  Â  _ , gemm_fp8_flops, attn_fp16_flops = mla_flops(q_len,kv_len,args,Â 1)
Â  Â  gemm_fp8_time = gemm_fp8_flops / gpu.get_fp8_flops(discount, comm_sm)
Â  Â  print("GEMM_FP8 Elapsed time(ms): %.3f"Â % gemm_fp8_time)
Â  Â  attn_fp16_time = attn_fp16_flops / gpu.get_fp16_flops(discount, comm_sm)
Â  Â  print("ATTN_FP16 Elapsed time(ms): %.3f"Â % attn_fp16_time)
Â  Â  total_time = gemm_fp8_time + attn_fp16_time
Â  Â  print("Total Elapsed time(ms):%.3f"Â % total_time)
Â  Â Â 
Â  Â  all_reduce_comm_size = seq_len * args.dim *Â 2Â /1024/1024Â Â #fp16 take 2Bytes
Â  Â  ar_elapsed_time = all_reduce_comm_size / gpu.get_nvlink_bw(discount)
Â  Â  print("AR Elapsed time(ms):%.3f"Â % ar_elapsed_time)
Â  Â Â 
Â  Â  tp4_time = total_time/4Â + ar_elapsed_time
Â  Â  print("TP4 Elapsed time(ms):%.3f"Â % tp4_time)
Â  Â Â 
Â  Â  tp8_time = total_time/8Â + ar_elapsed_time
Â  Â  print("TP8 Elapsed time(ms):%.3f"Â % tp8_time)
Â  Â Â returnÂ total_time, tp4_time,tp8_time

defÂ prefill_mla(args:ModelArgs, gpu_dict, seq_len, kv_cache_rate):
Â  Â  df = pd.DataFrame(columns=['GPU','TP1','TP4','TP8'])
Â  Â Â forÂ keyÂ inÂ gpu_dict.keys():
Â  Â  Â  Â  print('------------ %s --------------'Â % key)
Â  Â  Â  Â  tp1,tp4,tp8 = prefill_mla_elapse_time(args,gpu_dict[key], seq_len, kv_cache_rate)
Â  Â  Â  Â  df.loc[len(df)]=[key,tp1,tp4,tp8]
Â  Â  print(df.set_index('GPU').to_markdown(floatfmt=".3f"))


H800éœ€è¦24ä¸ªé€šä¿¡SM. è¿™é‡Œè€ƒè™‘åˆ°H20æµ®ç‚¹ç®—åŠ›æ¯”è¾ƒå¼±, H20ä¼°è®¡éœ€è¦10ä¸ªé€šä¿¡SM, åŒæ—¶è®¡ç®—äº†TP=4å’ŒTP=8ä¸¤ç§æƒ…å†µ, å¹¶åœ¨åé¢é’ˆå¯¹ä¸¤ç§å¹¶è¡Œç­–ç•¥çš„ååè¿›è¡Œåˆ†æ, å½“TP=8æ—¶, DPç»„å°†ä¼šå˜æˆ4ä¸ª.ä½¿ç”¨TPå¹¶è¡Œæ—¶, Allreduceé€šä¿¡é‡ä¸ºseq_lenÂ xÂ dimÂ xÂ 2Bytes(BF16)

seq_len =Â 4383
kv_cache_rate =Â 0.563

prefill_mla(args,gpu,seq_len,kv_cache_rate)
------------ H800 --------------
GEMM_FP8 Elapsed time(ms):Â 0.536
ATTN_FP16 Elapsed time(ms):Â 4.729
Total Elapsed time(ms):5.265
AR Elapsed time(ms):0.352
TP4 Elapsed time(ms):1.669
TP8 Elapsed time(ms):1.011
------------ H20 --------------
GEMM_FP8 Elapsed time(ms):Â 3.364
ATTN_FP16 Elapsed time(ms):Â 29.671
Total Elapsed time(ms):33.035
AR Elapsed time(ms):0.176
TP4 Elapsed time(ms):8.435
TP8 Elapsed time(ms):4.306



ç»Ÿè®¡MLAä¸­GPUçš„è®¡ç®—æ—¶é—´(å•ä½ms)ä¸º

GPU
	
TP1
	
TP4
	
TP8


H800
	
5.265
	
1.669
	
1.011


H20
	
33.035
	
8.435
	
4.306
2.2 DenseMLPè®¡ç®—è€—æ—¶

DenseMLPè¿ç®—é‡ç»Ÿè®¡å¦‚ä¸‹:

defÂ densmlp_flops(args:ModelArgs, seq_len):
Â  Â Â returnÂ 3Â * seq_len * args.dim * args.inter_dim *2/1e9
Â  Â Â 
defÂ dense_mlp_elapse_time(args:ModelArgs,gpu:GPU_perf, seq_len):
Â  Â  gemm_fp8_flops = densmlp_flops(args, seq_len)
Â  Â  gemm_fp8_time = gemm_fp8_flops / gpu.get_fp8_flops()
Â  Â  print("Elapsed time(ms): %.3f"Â % gemm_fp8_time)
Â  Â Â returnÂ gemm_fp8_time

defÂ prefill_dense_mlp(args:ModelArgs, gpu_dict, seq_len):
Â  Â  df = pd.DataFrame(columns=['GPU','DenseMLPè€—æ—¶'])
Â  Â Â forÂ keyÂ inÂ gpu_dict.keys():
Â  Â  Â  Â  print('------------ %s --------------'Â % key)
Â  Â  Â  Â  t = dense_mlp_elapse_time(args,gpu_dict[key], seq_len)
Â  Â  Â  Â  df.loc[len(df)]=[key,t]
Â  Â  print(df.set_index('GPU').to_markdown(floatfmt=".3f"))


å®é™…è¿ç®—çš„é•¿åº¦ä¸º

q_len = seq_len *( 1- kv_cache_rate)
------------ H800 --------------
Elapsed time(ms): 3.156
------------ H20 --------------
Elapsed time(ms): 19.801


DenseMLPç´¯è®¡è€—æ—¶(å•ä½ms):

GPU
	
DenseMLPè€—æ—¶


H800
	
3.156


H20
	
19.801
2.3 MoEè®¡ç®—è€—æ—¶

TP=4æ—¶, DP=8, é‚£ä¹ˆç›¸å½“äºMLAåŒæ—¶äº§ç”Ÿäº†8ç»„seq_lençš„token, å¹³å‡æ¯å¡Shared Expertè®¡ç®—çš„tokenæ•°ä¸ºÂ seq_len*Â dp_groupÂ /Â num_gpu

å¯¹äºRouted Expert, å½“topk=8æ—¶æ€»å…±éœ€è¦å¤„ç†çš„Routed Expertçš„è®¡ç®—é‡ä¸ºseq_lenÂ *Â dp_groupÂ *Â topk, ç„¶åå¹³å‡åˆ†æ‘Šåˆ°32å¡ä¸Š, æ¯å¡çš„Routed Expertè®¡ç®—é‡ä¸ºseq_lenÂ *Â dp_groupÂ *Â topkÂ /Â num_gpu

defÂ moe_expert_flops(args:ModelArgs, seq_len):
Â  Â Â returnÂ 3Â * seq_len * args.dim * args.moe_inter_dim *2/1e9

defÂ moe_expert_elapse_time(args:ModelArgs,gpu:GPU_perf, seq_len, tp, dp):
Â  Â  num_device = tp * dp
Â  Â  num_shared_token = dp * seq_len / num_device
Â  Â  shared_flops = moe_expert_flops(args, num_shared_token)
Â  Â  shared_time = shared_flops / gpu.get_fp8_flops()
Â  Â  print("Shared Expert Elapsed time(ms): %.3f"Â % shared_time)

Â  Â  num_routed_token = seq_len * dp * args.n_activated_experts / num_device
Â  Â  routed_flops = moe_expert_flops(args, num_routed_token)
Â  Â  routed_time = routed_flops / gpu.get_fp8_flops()
Â  Â  print("Routed Expert Elapsed time(ms): %.3f"Â % routed_time)

Â  Â Â returnÂ shared_time, routed_time

defÂ prefill_moe(args:ModelArgs, gpu_dict, seq_len, tp, dp ):
Â  Â  df = pd.DataFrame(columns=['GPU','Shared Expert','Routed Expert'])
Â  Â Â forÂ keyÂ inÂ gpu_dict.keys():
Â  Â  Â  Â  print('------------ %s --------------'Â % key)
Â  Â  Â  Â  s, r = moe_expert_elapse_time(args,gpu_dict[key], seq_len,tp,dp)
Â  Â  Â  Â  df.loc[len(df)]=[key,s,r]
Â  Â  print(df.set_index('GPU').to_markdown(floatfmt=".3f"))


TP=4æ—¶, DP = 8, GPUè€—æ—¶å¦‚ä¸‹q

prefill_moe(args,gpu, seq_len, tp=4,dp=8)
------------ H800 --------------
Shared Expert Elapsed time(ms):Â 0.088
Routed Expert Elapsed time(ms):Â 0.701
------------ H20 --------------
Shared Expert Elapsed time(ms):Â 0.550
Routed Expert Elapsed time(ms):Â 4.400


MoE Expertè®¡ç®—è€—æ—¶(å•ä½ms):

GPU
	
Shared Expert
	
Routed Expert


H800
	
0.088
	
0.701


H20
	
0.550
	
4.400

TP=8æ—¶, DP = 4, GPUè€—æ—¶å¦‚ä¸‹

prefill_moe(args,gpu, seq_len, tp=8,dp=4)
------------ H800 --------------
Shared Expert Elapsed time(ms):Â 0.044
Routed Expert Elapsed time(ms):Â 0.351
------------ H20 --------------
Shared Expert Elapsed time(ms):Â 0.275
Routed Expert Elapsed time(ms):Â 2.200


MoE Expertè®¡ç®—è€—æ—¶(å•ä½ms):

GPU
	
Shared Expert
	
Routed Expert


H800
	
0.044
	
0.351


H20
	
0.275
	
2.200
2.4 AlltoAllé€šä¿¡è€—æ—¶

DeepSeek-V3è®¾è®¡äº†MoE Groupçš„æ¦‚å¿µ, ç”¨äºå¹³è¡¡NVLINKå’ŒIBçš„å¸¦å®½. ä¸€ä¸ªTokené€šè¿‡MoE Gatingå‡½æ•°, ä¸€ä¸ªtokenæœ€å¤šä»…ä¼šåˆ†å‘åˆ°4ä¸ªèŠ‚ç‚¹ä¸Š. æŒ‰ç…§EPå¹¶è¡Œä¸“å®¶è´Ÿè½½å®Œå…¨å‡è¡¡çš„æƒ…å†µä¸‹è€ƒè™‘, åœ¨RDMAä¸Šçš„è·¨æœºé€šä¿¡ä¸º 3 * tokenæ•°. Dispatché€šä¿¡é‡ä¸º: TP=4: Â æ¯èŠ‚ç‚¹æœ‰2ä¸ªDPç»„,ç´¯è®¡éœ€è¦å‘é€ 2 * 3 *Â seq_lenÂ  *Â dimÂ . TP=8: Â æ¯èŠ‚ç‚¹æœ‰1ä¸ªDPç»„,ç´¯è®¡éœ€è¦å‘é€ 3 *Â seq_lenÂ  *Â dimÂ .

Combineé˜¶æ®µç”±äºæ•°æ®ä¸ºFP16, é€šä¿¡é‡ç¿»å€, H800å’ŒH20 ScaleOutå¸¦å®½ç›¸åŒ, æŒ‰ç…§DeepEPå¯ä»¥æ‰“æ»¡45GB/s, ä½†åŒæ—¶å…¼é¡¾æ€»å¸¦å®½åˆ©ç”¨ç‡80%~40GB/sè®¡ç®—, æ€»å¸¦å®½ä¸º 40GB/s * 8 = 320GB/s, é€šä¿¡è€—æ—¶ä¸º:

defÂ prefill_alltoall_time(args:ModelArgs, gpu, seq_len, dispatch_node, tp):
Â  Â Â ##é€šä¿¡é‡ä¼°è®¡
Â  Â  gpu_per_node =Â 8
Â  Â  dp = gpu_per_node/tp
Â  Â  dispatch_size = (dispatch_node -Â 1) * dp * seq_len * args.dim /1024/1024
Â  Â  combine_size =Â 2Â * dispatch_size Â #fp16 Â 
Â  Â  comm_bw = gpu.get_pcie_bw() * gpu_per_node
Â  Â  dispatch_time = dispatch_size / comm_bw
Â  Â  combine_time = combine_size / comm_bw
Â  Â Â returnÂ dispatch_time, combine_time


defÂ prefill_alltoall(args:ModelArgs, gpu_dict, seq_len, dispatch_node, tp):Â Â 
Â  Â  df = pd.DataFrame(columns=['GPU','Dispatch','Combine'])
Â  Â Â forÂ keyÂ inÂ gpu_dict.keys():
Â  Â  Â  Â  print('------------ %s --------------'Â % key)
Â  Â  Â  Â  dispatch_time, combine_time = prefill_alltoall_time(args, gpu_dict[key],seq_len, dispatch_node, tp)
Â  Â  Â  Â  print("Dispatch Elapsed time(ms): %.3f"Â % dispatch_time)
Â  Â  Â  Â  print("Combine Elapsed time(ms): %.3f"Â % combine_time) Â  Â  Â 
Â  Â  Â  Â  df.loc[len(df)]=[key,dispatch_time,combine_time]
Â  Â  print(df.set_index('GPU').to_markdown(floatfmt=".3f"))


åœ¨TP=4æ—¶, å•ä¸ªèŠ‚ç‚¹æœ‰2ä¸ªDPç»„, è®¡ç®—ç»“æœå¦‚ä¸‹(å•ä½ms):

prefill_alltoall(args,gpu,seq_len,dispatch_node=4,tp=4)
------------ H800 --------------
Dispatch Elapsed time(ms):Â 0.529
Combine Elapsed time(ms):Â 1.057
------------ H20 --------------
Dispatch Elapsed time(ms):Â 0.529
Combine Elapsed time(ms):Â 1.057

GPU
	
Dispatch
	
Combine


H800
	
0.529
	
1.057


H20
	
0.529
	
1.057

åœ¨TP=8æ—¶, å•ä¸ªèŠ‚ç‚¹åªæœ‰1ä¸ªDPç»„, è®¡ç®—ç»“æœå¦‚ä¸‹(å•ä½ms):

prefill_alltoall(args,gpu,seq_len,dispatch_node=4,tp=8)
------------ H800 --------------
Dispatch Elapsed time(ms):Â 0.264
Combine Elapsed time(ms):Â 0.529
------------ H20 --------------
Dispatch Elapsed time(ms):Â 0.264
Combine Elapsed time(ms):Â 0.529

GPU
	
Dispatch
	
Combine


H800
	
0.264
	
0.529


H20
	
0.264
	
0.529
2.5 æ€»è€—æ—¶

ç´¯è®¡è€—æ—¶, éOverlapè®¡ç®—

3x(MLA_tp1 + DenseMLP) + 58x(MLA_tpN + Shared Expert + Routed Expert +Dispatch + Combine)

å®Œå…¨Overlapè®¡ç®—

3x(MLA_tp1 + DenseMLP) + 58x(MLA_tpN + Shared Expert + Routed Expert)

å®šä¹‰è®¡ç®—å‡½æ•°å¦‚ä¸‹æ‰€ç¤º:

defÂ prefill_time(args:ModelArgs, gpu, seq_len, kv_cache_rate, tp , dp):
Â  Â  dispatch_node =Â 4
Â  Â  gpu_per_node =Â 8
Â  Â  num_device Â = Â tp * dp
Â  Â  dense_mla,tp4_mla,tp8_mla = prefill_mla_elapse_time(args, gpu, Â seq_len, kv_cache_rate)Â 
Â  Â  tp_mla = tp4_mlaÂ ifÂ tp ==Â 4Â elseÂ tp8_mla
Â  Â  dense_mlp = dense_mlp_elapse_time(args, gpu, seq_len)
Â  Â  shared, routed = moe_expert_elapse_time(args, gpu, seq_len, tp, dp)
Â  Â  dispatch, combine = prefill_alltoall_time(args, gpu, seq_len, dispatch_node, tp)
Â  Â Â returnÂ dense_mla, dense_mlp, tp_mla, shared, routed, dispatch, combine
Â  Â Â 
defÂ prefill_time_sum(args:ModelArgs, gpu_dict, seq_len, kv_cache_rate, tp , dp):
Â  Â  df = pd.DataFrame(columns=['MLA','DenseMLP','TP_MLA','Shared Expert','Routed Expert','Dispatch','Combine','GPU'])
Â  Â  df2 = pd.DataFrame(columns=['Sum(Overlap)','Sum','GPU'])
Â  Â  n_sparse_layers = args.n_layers - args.n_dense_layers
Â  Â  df.loc[len(df)]= [ args.n_dense_layers, args.n_dense_layers, Â #MLA+ DenseMLP
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â n_sparse_layers, n_sparse_layers, n_sparse_layers,Â #SparseLayer MLA + MoE
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â n_sparse_layers, n_sparse_layers,Â 'Layers']Â #Dispatch & Combine Layers
Â  Â Â forÂ keyÂ inÂ gpu_dict.keys():
Â  Â  Â  Â  t Â = list(prefill_time(args, gpu_dict[key], seq_len, kv_cache_rate , tp , dp))
Â  Â  Â  Â  t.append(key)
Â  Â  Â  Â  df.loc[len(df)]= t
Â  Â  Â  Â  sum_overlap = args.n_dense_layers * (t[0] + t[1]) + n_sparse_layers * ( t[2] + t[3] + t[4])Â 
Â  Â  Â  Â  sum_non_overlap = sum_overlap + n_sparse_layers * ( t[5] + t[6])Â #alltoall
Â  Â  Â  Â  df2.loc[len(df2)]= [ sum_overlap, sum_non_overlap, key]
Â  Â  df = df.set_index('GPU').T
Â  Â  df['Layers'] = df['Layers'].astype(int).astype(str)
Â  Â  print(df.to_markdown(floatfmt=".3f")) Â 
Â  Â  print('-----------SUM-------------')
Â  Â  df2 = df2.set_index('GPU').T
Â  Â  print(df2.to_markdown(floatfmt=".3f")) Â 
Â  Â Â 
Â  Â Â returnÂ df,df2


TP=4æ—¶, DP=8, è€—æ—¶åˆ†æå¦‚ä¸‹(å•ä½ms):

tp4_detail,tp4_sum = prefill_time_sum(args, gpu, seq_len, kv_cache_rate,tp=4Â , dp=8)



	
Layers
	
H800
	
H20


MLA
	
3
	
5.265
	
33.035


DenseMLP
	
3
	
3.156
	
19.801


TP_MLA
	
58
	
1.669
	
8.435


Shared Expert
	
58
	
0.088
	
0.550


Routed Expert
	
58
	
0.701
	
4.400


Dispatch
	
58
	
0.529
	
0.529


Combine
	
58
	
1.057
	
1.057

ç´¯è®¡æ—¶é—´åˆ†æ(å•ä½ms):



	
H800
	
H20


Sum(Overlap)
	
167.802
	
934.839


Sum
	
259.803
	
1026.840

TP=8æ—¶, DP=4, è€—æ—¶åˆ†æå¦‚ä¸‹(å•ä½ms):

tp8_detail,tp8_sum Â = prefill_time_sum(args, gpu, seq_len, kv_cache_rate,tp=8Â , dp=4)



	
Layers
	
H800
	
H20


MLA
	
3
	
5.265
	
33.035


DenseMLP
	
3
	
3.156
	
19.801


TP_MLA
	
58
	
1.011
	
4.306


Shared Expert
	
58
	
0.044
	
0.275


Routed Expert
	
58
	
0.351
	
2.200


Dispatch
	
58
	
0.264
	
0.264


Combine
	
58
	
0.529
	
0.529

ç´¯è®¡æ—¶é—´åˆ†æ(å•ä½ms):



	
H800
	
H20


Sum(Overlap)
	
106.754
	
551.784


Sum
	
152.754
	
597.784

ç”±äºç´¯è®¡ä¸ºDPç»„seq_lençš„æ¨ç†, å¹³å‡1så•æœºèƒ½å¤Ÿå¤„ç†çš„Tokenä¸ºÂ DP * seq_len * (1000ms / è®¡ç®—æ—¶é—´)/èŠ‚ç‚¹æ•°, è®¡ç®—å¦‚ä¸‹

å®˜æ–¹çš„TP=4çš„éƒ¨ç½²æ–¹å¼:

dp =Â 8
num_node =Â 4
print(tp4_sum.apply(lambdaÂ x: dp * seq_len * (1000/ x)/num_node).to_markdown(floatfmt=".1f"))



	
H800
	
H20


Sum(Overlap)
	
52240.1
	
9377.0


Sum
	
33741.0
	
8536.9

è€ŒTP=8çš„éƒ¨ç½²æ–¹å¼çš„åå:



	
H800
	
H20


Sum(Overlap)
	
41057.0
	
7943.3


Sum
	
28693.1
	
7332.1

å¯ä»¥çœ‹åˆ°DeepSeekå®˜æ–¹é€‰æ‹©çš„TP=4çš„é…ç½®æ˜¯ååæ›´ä¼˜çš„é€‰æ‹©, å¦å¤–å®˜æ–¹çš„æ•°æ®ä¸ºå•æœº73.7K tokens/s(å«ç¼“å­˜å‘½ä¸­), æŠ˜ç®—å‡ºæ¥éå‘½ä¸­éœ€è¦è®¡ç®—çš„å¹³å‡token/sä¸º 32207, è€ƒè™‘åˆ°æ¯å¤©çš„å³°è°·æ•ˆåº”, è¯¥å€¼ç¬¦åˆé¢„æœŸ.

å¦ä¸€æ–¹é¢è€ƒè™‘åˆ°H20å¯¹äºTTFTé¦–Tokenå»¶è¿Ÿçš„å½±å“, TP=4å·²ç»è¶…è¿‡1s, å¯ä»¥é‡‡ç”¨TP=8çš„ç­–ç•¥é™ä½é¦–Tokenå»¶è¿Ÿ.

2.6 Overlapåˆ†æ

åœ¨å®˜æ–¹éƒ¨ç½²æ–¹æ¡ˆä¸­, å¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ–¹å¼åˆ†ä¸¤ä¸ªMicro-batchè¿›è¡ŒOverlap
åŸºäºæ­¤å¯¹å®˜æ–¹çš„prefill.json traceæ ‡æ³¨å¦‚ä¸‹, å®é™…ä¸Šçš„Traceè¿˜æ˜¯æœ‰ä¸€äº›æ²¡æœ‰Overlapçš„:

å®é™…è®¡ç®—TP=4æ—¶, Prefillçš„è®¡ç®—è€—æ—¶å¦‚ä¸‹, å¯ä»¥çœ‹åˆ°é€šä¿¡æ˜¯å¯ä»¥è¢«è®¡ç®—Overlapçš„.



	
Layers
	
H800
	
H20


TP_MLA
	
58
	
1.669
	
8.435


Shared Expert
	
58
	
0.088
	
0.550


Combine
	
58
	
1.057
	
1.057


-
	
-
	
-
	
-


Routed Expert
	
58
	
0.701
	
4.400


Dispatch
	
58
	
0.529
	
0.529

ç‰¹åˆ«çš„æ¥çœ‹, H20ä¸­è¿˜å¯ä»¥é™ä½RDMA ScaleOutçš„å¸¦å®½, åšäº†ä¸€äº›åˆæ­¥çš„ä¼°è®¡

h20_32 = GPU_perf( sm =Â 78Â ,comm_sm =Â 10,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 96,mem_bw =Â 3350,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 50,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)

h20_16 = GPU_perf( sm =Â 78Â ,comm_sm =Â 10,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 96,mem_bw =Â 3350,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 25,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)

h20_8 = GPU_perf( sm =Â 78Â ,comm_sm =Â 10,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 96,mem_bw =Â 3350,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 12.5,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)

gpu_h20 = dict({Â 'H20-3.2T': h20_32,Â 'H20-1.6T': h20_16 ,Â 'H20-800G': h20_8})
tp4_detail,tp4_sum = prefill_time_sum(args, gpu_h20, seq_len, kv_cache_rate,tp=4Â , dp=8)



	
Layers
	
H20-3.2T
	
H20-1.6T
	
H20-800G


TP_MLA
	
58
	
8.435
	
8.435
	
8.435


Shared Expert
	
58
	
0.550
	
0.550
	
0.550


Dispatch
	
58
	
0.529
	
1.057
	
2.115


-
	
-
	
-
	
-
	
-


Routed Expert
	
58
	
4.400
	
4.400
	
4.400


Combine
	
58
	
1.057
	
2.115
	
4.230

å¯ä»¥çœ‹åˆ°åœ¨Prefillé˜¶æ®µ, å¦‚æœå¹³å‡seq_lenè¶³å¤Ÿé•¿æ—¶, 800Gä¹Ÿèƒ½å¾ˆå¥½çš„Overlap



	
H20-3.2T
	
H20-1.6T
	
H20-800G


Sum(Overlap)
	
934.839
	
934.839
	
934.839


Sum
	
1026.840
	
1118.841
	
1302.842
æ³¨:å¦‚æœå¤§é‡çš„Prefillé•¿åº¦åœ¨1000~2000å·¦å³ä¾æ—§éœ€è¦1.6Tbps~3.2Tbps RDMA.
2.7 KVCacheè®¡ç®—

å¯¹äºToken/sæˆ‘ä»¬è¿˜å¯ä»¥æŠ˜ç®—å‡ºä¼ è¾“KVCacheçš„æ€»é‡:

dp =Â 8
num_node =Â 4

tp4_detail,tp4_sum = prefill_time_sum(args, gpu, seq_len, kv_cache_rate,tp=4Â , dp=8)
kvcache_fp8 = tp4_sum.apply(lambdaÂ x: dp * seq_len * (1000/ x)/num_node * (args.kv_lora_rank + args.qk_rope_head_dim)/1024/1024)
kvcache_fp16 = kvcache_fp8 *2
kvcache=kvcache_fp8.join(kvcache_fp16, lsuffix='(FP8)',rsuffix='(FP16)')
print(kvcache.to_markdown(floatfmt=".1f"))

GB/s
	
H800(FP8)
	
H20(FP8)
	
H800(FP16)
	
H20(FP16)


Sum(Overlap)
	
28.7
	
5.2
	
57.4
	
10.3


Sum
	
18.5
	
4.7
	
37.1
	
9.4
æ³¨: è¿™é‡Œæ²¡æœ‰è€ƒè™‘KVCacheå‘½ä¸­ç‡, è€ƒè™‘ååº”è¯¥å¸¦å®½æŠ˜ç®—ä¸ºè¯»å†™ä¸¤ä¸ªæ–¹å‘.

å¯¹äºH800, å¦‚æœKV-Cacheé‡‡ç”¨FP16å­˜å‚¨,åˆ™å·²ç»è¶…è¿‡äº†è¿æ¥CPUçš„é‚£å¼ 400Gbps(50GB/s)ç½‘å¡å¸¦å®½, éœ€è¦é‡‡ç”¨GPUç›´è¿çš„RDMA Scaleoutç½‘ç»œè¿›è¡Œä¼ è¾“. æ­£å¥½å‰å‡ å¤©å’Œå¤Coreè°ˆåˆ°è¿™ä¸ªé—®é¢˜, å®é™…ä¸Šå­˜å‚¨æ¥å…¥GPUäº’è”çš„ScaleOutç½‘ç»œ, å¹³å‡åˆ†æ‘Šåˆ°æ¯å¼ ç½‘å¡ä¸Š,é€‚å½“çš„ç¼–æ’é€šä¿¡ç®—å­, å¯¹äºEPå¹¶è¡Œå½±å“æ˜¯å‡ ä¹å¯ä»¥å¿½ç•¥çš„...

3. Decodingé˜¶æ®µ

Decodingé›†ç¾¤é‡‡ç”¨18å°éƒ¨ç½², è·¯ç”±ä¸“å®¶EP144, MLAå’Œå…±äº«ä¸“å®¶DP144. 32ä¸ªå†—ä½™è·¯ç”±ä¸“å®¶, æ¯å¼ å¡2ä¸ªè·¯ç”±ä¸“å®¶å’Œ1ä¸ªå…±äº«ä¸“å®¶.è€Œè®ºæ–‡çš„åšæ³•æ˜¯40å°éƒ¨ç½²EP320, æ¯å¼ å¡1ä¸ªä¸“å®¶, TP=4, DP=80, Decodeé˜¶æ®µä¸éœ€è¦ç‹¬ç«‹çš„é€šä¿¡SM, å› æ­¤å¯¹GPUæ€§èƒ½æ•°æ®å»ºæ¨¡å¦‚ä¸‹:

h800 = GPU_perf( sm =Â 132Â ,comm_sm =Â 0,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 791.6, fp8_flops =Â 1583.2,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 80,mem_bw =Â 3350,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 200,pcie_bw =Â 50,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)

h20 = GPU_perf( sm =Â 78Â ,comm_sm =Â 0,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 96,mem_bw =Â 3350,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 50,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)
h20_3e = GPU_perf( sm =Â 78Â ,comm_sm =Â 0,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 141,mem_bw =Â 4800,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 50,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)

gpu_decode = dict({'H800': h800,Â 'H20': h20,'H20_3e': h20_3e})
gpu_decode2 = dict({'H800': h800,Â 'H20': h20})Â 

3.1 EPç­–ç•¥åˆ†æ

æˆ‘ä»¬éœ€è¦æ ¹æ®EPå¹¶è¡Œç­–ç•¥æ¥åˆ†æ, å‡è®¾é›†ç¾¤çš„å¹¶è¡Œç­–ç•¥ä¸ºä¸ºDecodingé›†ç¾¤çš„æ€»çš„å¡æ•°,Â ä¸ºå†—ä½™ä¸“å®¶æ•°.å¹³å‡æ¯å¡çš„è·¯ç”±ä¸“å®¶æ•°ä¸ºæ»¡è¶³

ç”±äºçº¿ä¸Šç¯å¢ƒä¸­è¿˜æ˜¯æœ‰å¤§é‡çš„Expertè´Ÿè½½ä¸å‡è¡¡çš„æƒ…å†µ, éœ€è¦ä¿è¯æœ‰è¶³å¤Ÿå¤šçš„å†—ä½™ä¸“å®¶æ•°é‡ç”¨äºEPLBè°ƒåº¦, ä¾‹å¦‚æˆ‘ä»¬å®šä¹‰å†—ä½™ä¸“å®¶æ•°é‡ä¸èƒ½å°‘äº16, å¯ä»¥æ„å¸¸è§çš„å‡ ç§EPå¹¶è¡Œç­–ç•¥å¦‚ä¸‹:



	
å†—ä½™ä¸“å®¶
	
æ¯å¡ä¸“å®¶


EP34
	
16
	
8


EP72
	
32
	
4


EP144
	
32
	
2


EP320
	
64
	
1

å¯¹äºä¸åŒå¹¶è¡Œç­–ç•¥éœ€è¦é€šä¿¡å’Œå¤„ç†çš„Tokenæ•°æŒ‰å¦‚ä¸‹æ–¹å¼è®¡ç®—

classÂ MoE_EP():
Â  Â Â defÂ __init__(self,args:ModelArgs,ep_num, redundant_exp):
Â  Â  Â  Â  self.ep_num = ep_num
Â  Â  Â  Â  self.redundant_exp = redundant_exp
Â  Â  Â  Â  self.dispatch_num = args.n_activated_experts
Â  Â  Â  Â  self.n_routed_experts = args.n_routed_experts
Â  Â  Â  Â  self.expert_num = (args.n_routed_experts + redundant_exp) / self.ep_num

Â  Â Â defÂ expert_per_gpu(self):
Â  Â  Â  Â Â returnÂ self.expert_num
Â  Â  Â  Â Â 
Â  Â Â defÂ total_tokens(self,bs):
Â  Â  Â  Â Â returnÂ bs * self.ep_num

Â  Â Â defÂ comm_tokens(self, bs):
Â  Â  Â  Â Â #å¹³å‡æ¯ä¸ªtokenæœ‰self.expert_num / self.n_routed_expertsæ¦‚ç‡æœ¬åœ°å¤„ç†Â 
Â  Â  Â  Â Â returnÂ bs * self.dispatch_num *(1- self.expert_num / self.n_routed_experts)
Â  Â  Â  Â Â 
Â  Â Â defÂ compute_tokens(self, bs):
Â  Â  Â  Â Â #æ€»tokenæ•°ä¸ºbs * dispatch_num * ep_num, å¹³æ‘Šåˆ°æ¯å¼ å¡/ep_num
Â  Â  Â  Â Â returnÂ bs * self.dispatch_num Â 

ep_dict = {Â 'EP34': MoE_EP(args,Â 34,16),
Â  Â  Â  Â  Â  Â Â 'EP72'Â :MoE_EP(args,Â 72,32),
Â  Â  Â  Â  Â  Â Â 'EP144'Â :MoE_EP(args,Â 144,32),
Â  Â  Â  Â  Â  Â Â 'EP320'Â :MoE_EP(args,Â 320,64)}

3.2 Memoryåˆ©ç”¨ç‡åˆ†æ

æˆ‘ä»¬å…ˆä»¥Memoryå®¹é‡è¿›è¡Œåˆ†æ, å¾—å‡ºå„ç§å¹¶è¡Œåœºæ™¯ä¸‹çš„æœ€å¤§BatchSize. å¯¹äºæ¨¡å‹çš„å‚æ•°å‚è€ƒ1.4ç« èŠ‚, å› ä¸ºDecodingé˜¶æ®µè¦é‡‡ç”¨matabsorbçš„MLA, å› æ­¤é™¤å»MLAå’ŒExpertçš„å‚æ•°ä¸º671.16B - MLA(187.17M)* 61 - Expert(44.05M)* (256-Routed+1-Shared) * 58= 3.13Bå‚æ•°. æŠ˜ç®—æˆå®é™…æ˜¾å­˜æ¶ˆè€—ä¸º3.13 *(1000/1024)^3Â = 2.91GB

BatchSizeè®¡ç®—å¦‚ä¸‹,è€ƒè™‘Decodingçš„é•¿åº¦ä¸º1210, æ ¹æ®å‰ä¸€èŠ‚EPç­–ç•¥åˆ†æä¼°è®¡ä¸“å®¶æ•°

defÂ _decoding_batchsize(args:ModelArgs, gpu:GPU_perf, seq_len,decode_len,tp, expert_num, absorb=True, kvcache_fp16=False):
Â  Â  mem_util_rate =Â 0.9Â #torch/activationç­‰å…¶å®ƒå¼€é”€çš„æŠ˜æ‰£
Â  Â  mla =Â 598.14Â ifÂ absorbÂ elseÂ 187.17Â #MLAçš„å‚æ•°(å•ä½M)
Â  Â  expert_mem =Â 44.05Â #expertçš„å‚æ•°(å•ä½M)
Â  Â  others_parameter =Â 2.91Â #å…¶å®ƒå‚æ•°2.91GB
Â  Â  kv_cache = (seq_len+decode_len) * (args.kv_lora_rank + args.qk_rope_head_dim) *args.n_layers *tp
Â  Â Â ifÂ kvcache_fp16 :
Â  Â  Â  Â  kv_cache *=2
Â  Â  mem = gpu.mem * mem_util_rate - others_parameter - mla * args.n_layers/tp/1024
Â  Â  mem -= expert_mem *(args.n_layers - args.n_dense_layers) * expert_num /1024
Â  Â Â returnÂ mem *Â 1024Â *Â 1024Â *Â 1024Â / kv_cache

defÂ decode_batchsize(args:ModelArgs, gpu_dict, seq_len,decode_len, tp):
Â  Â  df = pd.DataFrame(columns=['GPU','EP320','EP144','EP72','EP34'])
Â  Â Â forÂ fp16_kvcacheÂ inÂ range(0,2):
Â  Â  Â  Â Â forÂ keyÂ inÂ gpu_dict.keys():
Â  Â  Â  Â  Â  Â Â forÂ absorbÂ inÂ range(0,2):
Â  Â  Â  Â  Â  Â  Â  Â  item = key
Â  Â  Â  Â  Â  Â  Â  Â Â ifÂ bool(fp16_kvcache):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  item +='_FP16'
Â  Â  Â  Â  Â  Â  Â  Â Â else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  item +='_FP8'
Â  Â  Â  Â  Â  Â  Â  Â Â ifÂ bool(absorb):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  item +='_Absorb'Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  value = [item]
Â  Â  Â  Â  Â  Â  Â  Â Â forÂ exp_numÂ inÂ [2,3,5,9]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  bs = _decoding_batchsize(args, gpu_dict[key], seq_len,decode_len, tp,exp_num, bool(absorb),bool(fp16_kvcache))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value.append(bs)
Â  Â  Â  Â  Â  Â  Â  Â  df.loc[len(df)]= value
Â  Â  print(df.set_index('GPU').to_markdown(floatfmt=".0f")) Â 
Â  Â Â returnÂ df

decode_len =Â 1210
df = decode_batchsize(args,gpu_decode, seq_len,decode_len, tp=1)


ä¸åŒçš„å¹¶è¡Œç­–ç•¥ç»“æœå¯ä»¥æ‰¿è½½çš„BatchSizeå¦‚ä¸‹æ‰€ç¤º:

MLA:TP=1
	
EP320
	
EP144
	
EP72
	
EP34


H800_FP8
	
289
	
276
	
248
	
194


H800_FP8_Absorb
	
156
	
142
	
115
	
60


H20_FP8
	
368
	
354
	
327
	
273


H20_FP8_Absorb
	
234
	
221
	
193
	
139


H20_3e_FP8
	
589
	
576
	
548
	
494


H20_3e_FP8_Absorb
	
456
	
442
	
415
	
360


H800_FP16
	
145
	
138
	
124
	
97


H800_FP16_Absorb
	
78
	
71
	
57
	
30


H20_FP16
	
184
	
177
	
164
	
136


H20_FP16_Absorb
	
117
	
110
	
97
	
69


H20_3e_FP16
	
295
	
288
	
274
	
247


H20_3e_FP16_Absorb
	
228
	
221
	
207
	
180

TP=4æ—¶, èƒ½å¤Ÿæ‰¿å—çš„BatchSize

GPU
	
EP320
	
EP144
	
EP72
	
EP34


H800_FP8
	
84
	
80
	
74
	
60


H800_FP8_Absorb
	
75
	
72
	
65
	
52


H20_FP8
	
103
	
100
	
93
	
80


H20_FP8_Absorb
	
95
	
92
	
85
	
71


H20_3e_FP8
	
159
	
155
	
149
	
135


H20_3e_FP8_Absorb
	
150
	
147
	
140
	
127


H800_FP16
	
42
	
40
	
37
	
30


H800_FP16_Absorb
	
38
	
36
	
33
	
26


H20_FP16
	
52
	
50
	
47
	
40


H20_FP16_Absorb
	
48
	
46
	
42
	
36


H20_3e_FP16
	
79
	
78
	
74
	
67


H20_3e_FP16_Absorb
	
75
	
73
	
70
	
63

TP=8æ—¶, èƒ½å¤Ÿæ‰¿å—çš„BatchSize

GPU
	
EP320
	
EP144
	
EP72
	
EP34


H800_FP8
	
43
	
41
	
38
	
31


H800_FP8_Absorb
	
41
	
39
	
36
	
29


H20_FP8
	
53
	
51
	
48
	
41


H20_FP8_Absorb
	
51
	
49
	
45
	
39


H20_3e_FP8
	
80
	
79
	
75
	
68


H20_3e_FP8_Absorb
	
78
	
77
	
73
	
66


H800_FP16
	
21
	
21
	
19
	
15


H800_FP16_Absorb
	
20
	
20
	
18
	
14


H20_FP16
	
26
	
25
	
24
	
20


H20_FP16_Absorb
	
25
	
24
	
23
	
19


H20_3e_FP16
	
40
	
39
	
38
	
34


H20_3e_FP16_Absorb
	
39
	
38
	
37
	
33

ç»“è®º: ä»å†…å­˜å®¹é‡çš„è§’åº¦çœ‹, æ›´å¤§çš„æ˜¾å­˜æ›´å®¹æ˜“æ”¾ä¸‹è¶³å¤Ÿçš„BatchSize, ç”±äºDecodingçš„ç®—åŠ›å½±å“, éœ€è¦è€ƒè™‘MLAçŸ©é˜µå¸æ”¶åçš„å¸¦å®½å ç”¨, ä½†æ•´ä¸ªEPå¹¶è¡Œç­–ç•¥éœ€è¦ä¿è¯æ¯ä¸ªå¡çš„è·¯ç”±ä¸“å®¶æ•°ä¸è¶…è¿‡8. å¦å¤–,å¯¹äºH800å¦‚æœåœ¨MLAçŸ©é˜µå¸æ”¶æ¨¡å¼ä¸‹è¿è¡Œ, è¿˜éœ€è¦ä¿è¯KVCacheæŒ‰ç…§FP8å­˜å‚¨æ‰èƒ½æ»¡è¶³batchsize=128çš„éœ€æ±‚.

3.3 MLAè€—æ—¶

Decodingé˜¶æ®µæˆ‘ä»¬é‡‡ç”¨å¸¦çŸ©é˜µå¸æ”¶çš„MLAè®¡ç®—æ–¹å¼, ç”±äºè®¡ç®—å»¶è¿Ÿè¾ƒä½, æˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘åŠ è½½KVCacheçš„æ—¶é—´, è®¡ç®—æ–¹å¼å¦‚ä¸‹

bs_list =[32,Â 64,Â 128,Â 256]

defÂ decode_mla_elapse_time(args:ModelArgs, gpu:GPU_perf, seq_len, bs, absorb=True):
Â  Â  mla_flops_func = mla_matabsob_flopsÂ ifÂ absorbÂ elseÂ mla_flops
Â  Â Â #Decodingæ—¶è®¡ç®—ä¸ºqlen=1, kv_cache_rate = 1
Â  Â  _ , gemm_fp8_flops, attn_fp16_flops = mla_flops_func(1,seq_len,args,Â 1)
Â  Â Â 
Â  Â  gemm_fp8_time = gemm_fp8_flops / gpu.get_fp8_flops() * bs
Â  Â  print("GEMM_FP8 Elapsed time(ms): %.3f"Â % gemm_fp8_time)
Â  Â  attn_fp16_time = attn_fp16_flops / gpu.get_fp16_flops() *bs
Â  Â  print("ATTN_FP16 Elapsed time(ms): %.3f"Â % attn_fp16_time)Â 
Â  Â  total_time = gemm_fp8_time + attn_fp16_time
Â  Â  print("Total Elapsed time(ms):%.3f"Â % total_time)
Â  Â  all_reduce_comm_size = seq_len * args.dim *Â 2Â /1024/1024Â Â #fp16 take 2Bytes
Â  Â  ar_elapsed_time = all_reduce_comm_size / gpu.get_nvlink_bw()
Â  Â  print("AR Elapsed time(ms):%.3f"Â % ar_elapsed_time)
Â  Â  tp4_time = total_time/4Â + ar_elapsed_time
Â  Â  print("TP4 Elapsed time(ms):%.3f"Â % tp4_time)
Â  Â  tp8_time = total_time/8Â + ar_elapsed_time
Â  Â  print("TP8 Elapsed time(ms):%.3f"Â % tp8_time)
Â  Â Â returnÂ total_time, tp4_time, tp8_time

defÂ decode_kvcache_load_time(args:ModelArgs, gpu:GPU_perf, seq_len, bs):
Â  Â  kv_cache = seq_len * (args.kv_lora_rank + args.qk_rope_head_dim) Â * bsÂ 
Â  Â  load_kv_time = kv_cache /1024/1024/1024Â / gpu.get_mem_bw() *1000
Â  Â Â returnÂ load_kv_time Â  Â Â 

defÂ decode_mla(args:ModelArgs, gpu_dict, seq_len,absorb=True):
Â  Â  df = pd.DataFrame(columns=['GPU','BatchSize','TP1','TP4','TP8','LoadKV_FP8','LoadKV_FP16'])
Â  Â Â forÂ keyÂ inÂ gpu_dict.keys():
Â  Â  Â  Â Â forÂ bsÂ inÂ bs_list:Â 
Â  Â  Â  Â  Â  Â  Â tp1, tp4,tp8 = decode_mla_elapse_time(args,gpu_dict[key], seq_len, bs,absorb)
Â  Â  Â  Â  Â  Â  Â kv = decode_kvcache_load_time(args,gpu_dict[key], seq_len, bs)
Â  Â  Â  Â  Â  Â  Â df.loc[len(df)]= [key, bs,tp1,tp4,tp8,kv, kv*2]
Â  Â  Â  Â  Â  Â  Â df['BatchSize'] = df['BatchSize'].astype(int).astype(str)
Â  Â  print(df.set_index('GPU').to_markdown(floatfmt=".3f")) Â 
Â  Â Â returnÂ df

decode_mla(args,gpu_decode,seq_len)


è®¡ç®—ç»“æœ(å•ä½ms):

GPU
	
BatchSize
	
TP1
	
TP4
	
TP8
	
LoadKV_FP8
	
LoadKV_FP16


H800
	
32
	
0.109
	
0.380
	
0.366
	
0.026
	
0.053


H800
	
64
	
0.217
	
0.407
	
0.380
	
0.053
	
0.106


H800
	
128
	
0.435
	
0.461
	
0.407
	
0.106
	
0.211


H800
	
256
	
0.869
	
0.570
	
0.461
	
0.211
	
0.423


H20
	
32
	
0.726
	
0.358
	
0.267
	
0.026
	
0.053


H20
	
64
	
1.453
	
0.539
	
0.358
	
0.053
	
0.106


H20
	
128
	
2.906
	
0.903
	
0.539
	
0.106
	
0.211


H20
	
256
	
5.811
	
1.629
	
0.903
	
0.211
	
0.423

å¦‚æœä¸é‡‡ç”¨çŸ©é˜µå¸æ”¶æ¨¡å¼, è®¡ç®—ç»“æœå¦‚ä¸‹(å•ä½ms)

GPU
	
BatchSize
	
TP1
	
TP4
	
TP8
	
LoadKV_FP8
	
LoadKV_FP16


H800
	
32
	
3.528
	
1.234
	
0.793
	
0.026
	
0.053


H800
	
64
	
7.055
	
2.116
	
1.234
	
0.053
	
0.106


H800
	
128
	
14.111
	
3.880
	
2.116
	
0.106
	
0.211


H800
	
256
	
28.222
	
7.408
	
3.880
	
0.211
	
0.423


H20
	
32
	
23.586
	
6.073
	
3.124
	
0.026
	
0.053


H20
	
64
	
47.172
	
11.969
	
6.073
	
0.053
	
0.106


H20
	
128
	
94.343
	
23.762
	
11.969
	
0.106
	
0.211


H20
	
256
	
188.686
	
47.348
	
23.762
	
0.211
	
0.423

å› æ­¤æ— è®ºæ˜¯H800è¿˜æ˜¯H20åœ¨Decodingé˜¶æ®µéƒ½éœ€è¦é‡‡ç”¨çŸ©é˜µå¸æ”¶çš„MLAè®¡ç®—æ¨¡å¼, å¯¹äºH800 MLAè®¡ç®—æ—¶, BatchSize=128æ—¶, TP=4çš„å¹¶è¡Œç­–ç•¥å’ŒTP1è€—æ—¶ç›¸è¿‘, BatchSize=64çš„æ—¶å€™è¿˜æ›´å¿«, å› æ­¤åœ¨EP144çš„éƒ¨ç½²ä¸­æ²¡æœ‰ä½¿ç”¨TPå¹¶è¡Œ,è€Œåœ¨EP320çš„éƒ¨ç½²ä¸­, å¦‚æœBatchSize=256æ—¶, ä½¿ç”¨TP=4å¹¶è¡Œæœ‰æ”¶ç›Š.

è€Œé’ˆå¯¹H20çš„æœ€ä½³å®è·µæ˜¯, MLAçš„è®¡ç®—å¿…é¡»è¦ä½¿ç”¨TPå¹¶è¡Œ. ä½†æ˜¯TPå¹¶è¡Œè¿˜ä¼šå¯¼è‡´é¢å¤–çš„KVCacheå¼€é”€, å› æ­¤éœ€è¦æ ¸ç®—æ˜¾å­˜åˆ©ç”¨ç‡, å¯¹äºH20, è™½ç„¶TP=8å¯ä»¥æ˜¾è‘—é™ä½è¿ç®—å»¶è¿Ÿ, ä½†æ˜¯ä¹Ÿä¼šä½¿å¾—æœ€å¤§BatchSizeå—åˆ°çº¦æŸ, å› æ­¤H20æœ€ä¼˜çš„ç­–ç•¥ä¸ºTP=4

3.4 DenseMLPè€—æ—¶

è®¡ç®—æ–¹æ³•å¦‚ä¸‹æ‰€ç¤º, ä¸»è¦è€ƒè™‘ä¸åŒçš„BatchSizeä¸‹çš„è®¡ç®—å»¶è¿Ÿ:

defÂ decode_dense_mlp(args:ModelArgs, gpu_dict):
Â  Â  df = pd.DataFrame(columns=['GPU','BatchSize','DenseMLP'])
Â  Â Â forÂ keyÂ inÂ gpu_dict.keys():
Â  Â  Â  Â Â forÂ bsÂ inÂ bs_list:Â 
Â  Â  Â  Â  Â  Â  t = dense_mlp_elapse_time(args,gpu_dict[key], bs)
Â  Â  Â  Â  Â  Â  df.loc[len(df)]=[key,bs,t]
Â  Â  df['BatchSize'] = df['BatchSize'].astype(int).astype(str) Â  Â  Â  Â 
Â  Â  print(df.set_index('GPU').to_markdown(floatfmt=".3f"))
Â  Â Â returnÂ df
Â  Â Â 
decode_dense_mlp(args,gpu_decode)


è®¡ç®—è€—æ—¶å¦‚ä¸‹:

GPU
	
BatchSize
	
DenseMLP


H800
	
32
	
0.019


H800
	
64
	
0.038


H800
	
128
	
0.075


H800
	
256
	
0.151


H20
	
32
	
0.126


H20
	
64
	
0.252


H20
	
128
	
0.504


H20
	
256
	
1.008
3.5 MoEè€—æ—¶è®¡ç®—

æ ¹æ®ä¸åŒçš„GPUç±»å‹è®¡ç®—è€—æ—¶, å…¶å®è¿™ä¸ªå’ŒEPç­–ç•¥æ— å…³, å› ä¸ºä»»ä½•ä¸€ä¸ªtokenéƒ½è¦dispatch 8ä»½å‘åˆ°å…¶å®ƒèŠ‚ç‚¹, å› æ­¤ç®€åŒ–è®¡ç®—æµç¨‹, åŒæ—¶è¿˜éœ€è¦è€ƒè™‘åˆ°GroupGEMMå’Œç›¸å¯¹è¾ƒå°çš„batchsizeæ— æ³•æ‰“æ»¡çš„å½±å“, è¿™é‡ŒæŒ‰ç…§DeepGEMMçš„æ€§èƒ½, å®šä¹‰äº†ä¸€ä¸ªæ€§èƒ½æŠ˜ç®—ä¼°è®¡ç³»æ•°0.7.

defÂ _moe_expert_time(args:ModelArgs,gpu:GPU_perf,bs):
Â  Â  group_gemm_discount_rate =Â 0.7
Â  Â  shared_flops = moe_expert_flops(args, bs)
Â  Â  shared_time = shared_flops / gpu.get_fp8_flops() / group_gemm_discount_rate

Â  Â  num_routed_token = bs * args.n_activated_experts
Â  Â  routed_flops = moe_expert_flops(args, num_routed_token)
Â  Â  routed_time = routed_flops / gpu.get_fp8_flops() / group_gemm_discount_rate
Â  Â Â returnÂ shared_time, routed_time

defÂ moe_expert_time(args:ModelArgs,gpu_dict):
Â  Â  df = pd.DataFrame(columns=['GPU','BatchSize','SharedExpert','RoutedExpert'])
Â  Â Â forÂ gpu_keyÂ inÂ gpu_dict.keys():
Â  Â  Â  Â Â forÂ bsÂ inÂ bs_list:Â 
Â  Â  Â  Â  Â  Â  s, r = _moe_expert_time(args,gpu_dict[gpu_key], bs)
Â  Â  Â  Â  Â  Â  df.loc[len(df)]=[gpu_key,str(bs),s,r]
Â  Â  print(df.set_index('GPU').to_markdown(floatfmt=".3f")) Â  Â  Â  Â 
Â  Â Â returnÂ df

moe_expert_time(args,gpu_decode)


å„ç§ç»„åˆçš„ç»“æœå¦‚ä¸‹æ‰€ç¤º(å•ä½ms):

GPU
	
BatchSize
	
SharedExpert
	
RoutedExpert


H800
	
32
	
0.003
	
0.024


H800
	
64
	
0.006
	
0.048


H800
	
128
	
0.012
	
0.096


H800
	
256
	
0.024
	
0.191


H20
	
32
	
0.020
	
0.160


H20
	
64
	
0.040
	
0.320


H20
	
128
	
0.080
	
0.640


H20
	
256
	
0.160
	
1.280
3.5 AlltoAllé€šä¿¡è€—æ—¶

AlltoAllç”±äºé‡‡ç”¨IBGDAçš„æ–¹å¼, ç›´æ¥é€šè¿‡RDMAä¼ è¾“, å› æ­¤è®¡ç®—æ—¶ä»…éœ€è¦è€ƒè™‘GPUçš„PCIeå¸¦å®½, è®¡ç®—å‡½æ•°å¦‚ä¸‹æ‰€ç¤º:

defÂ _moe_a2a(args:ModelArgs,gpu:GPU_perf,bs):
Â  Â  dispatch_size = bs * args.dim * args.n_activated_experts /1024/1024Â 
Â  Â  combine_size = dispatch_size *Â 2Â #FP16
Â  Â  dispatch_t = dispatch_size / gpu.get_pcie_bw()
Â  Â  combine_t = combine_size / gpu.get_pcie_bw()
Â  Â Â returnÂ dispatch_t, combine_t


defÂ decode_a2a(args:ModelArgs, gpu_dict):Â Â 
Â  Â  df = pd.DataFrame(columns=['GPU','BatchSize','Dispatch','Combine'])
Â  Â Â forÂ keyÂ inÂ gpu_dict.keys():
Â  Â  Â  Â Â forÂ bsÂ inÂ [64,Â 128,Â 256]:Â 
Â  Â  Â  Â  Â  Â  dispatch_time, combine_time = _moe_a2a(args, gpu_dict[key],bs)
Â  Â  Â  Â  Â  Â  df.loc[len(df)]=[key,str(bs),dispatch_time,combine_time]
Â  Â  print(df.set_index('GPU').to_markdown(floatfmt=".3f"))

decode_a2a(args,gpu_decode)


Dispatchå’ŒCombineçš„è®¡ç®—ç»“æœä¸º(å•ä½ms):

GPU
	
BatchSize
	
Dispatch
	
Combine


H800
	
32
	
0.041
	
0.082


H800
	
64
	
0.082
	
0.165


H800
	
128
	
0.165
	
0.329


H800
	
256
	
0.329
	
0.659


H20
	
32
	
0.041
	
0.082


H20
	
64
	
0.082
	
0.165


H20
	
128
	
0.165
	
0.329


H20
	
256
	
0.329
	
0.659
3.6 æ€»è€—æ—¶

ç»Ÿè®¡æ€»è€—æ—¶è¡¨çš„å‡½æ•°å¦‚ä¸‹, ç”±äºH20å’ŒH20-3eåœ¨åŠ è½½KV-Cacheæ—¶æœ‰å¾ˆå°çš„æ€§èƒ½å·®è·, æ¥ä¸‹æ¥çš„è®¡ç®—ä»…è®¡ç®—H800å’ŒH20

fromÂ functoolsÂ importÂ reduce

defÂ _decoding_time(args:ModelArgs, gpu:GPU_perf,seq_len):
Â  Â  mla = decode_mla(args,gpu,seq_len)
Â  Â  dense_mlp = decode_dense_mlp(args,gpu)Â 
Â  Â  moe = moe_expert_time(args,gpu)
Â  Â  a2a = decode_a2a(args,gpu)
Â  Â  dfs = [ mla, dense_mlp, moe, a2a]
Â  Â  df = reduce(lambdaÂ left, right: pd.merge(left,right, on=['GPU','BatchSize'], how='left'), dfs)
Â  Â  print(df.set_index('GPU').T.to_markdown(floatfmt=".3f"))
Â  Â Â returnÂ df
Â  Â Â 
dfs = _decoding_time(args,gpu_decode2,seq_len)


ç»Ÿè®¡ç»“æœå¦‚ä¸‹æ‰€ç¤º(å•ä½ms):



	
H800
	
H800
	
H800
	
H800
	
H20
	
H20
	
H20
	
H20


BatchSize
	
32.000
	
64.000
	
128.000
	
256.000
	
32.000
	
64.000
	
128.000
	
256.000


TP1
	
0.109
	
0.217
	
0.435
	
0.869
	
0.726
	
1.453
	
2.906
	
5.811


TP4
	
0.380
	
0.407
	
0.461
	
0.570
	
0.358
	
0.539
	
0.903
	
1.629


TP8
	
0.366
	
0.380
	
0.407
	
0.461
	
0.267
	
0.358
	
0.539
	
0.903


LoadKV_FP8
	
0.026
	
0.053
	
0.106
	
0.211
	
0.026
	
0.053
	
0.106
	
0.211


LoadKV_FP16
	
0.053
	
0.106
	
0.211
	
0.423
	
0.053
	
0.106
	
0.211
	
0.423


DenseMLP
	
0.019
	
0.038
	
0.075
	
0.151
	
0.126
	
0.252
	
0.504
	
1.008


SharedExpert
	
0.003
	
0.006
	
0.012
	
0.024
	
0.020
	
0.040
	
0.080
	
0.160


RoutedExpert
	
0.024
	
0.048
	
0.096
	
0.191
	
0.160
	
0.320
	
0.640
	
1.280


Dispatch
	
0.041
	
0.082
	
0.165
	
0.329
	
0.041
	
0.082
	
0.165
	
0.329


Combine
	
0.082
	
0.165
	
0.329
	
0.659
	
0.082
	
0.165
	
0.329
	
0.659

æˆ‘ä»¬é’ˆå¯¹æ¨¡å‹ç»“æ„å’Œæœ€ä¼˜TPç­–ç•¥è¿›è¡Œä¿®æ­£,å¹¶è®¡ç®—TPOT,å¦‚ä¸‹æ‰€ç¤º:

defÂ decoding_time(args:ModelArgs, gpu_dict,seq_len):
Â  Â  df = _decoding_time(args,gpu_dict,seq_len)
Â  Â Â defÂ mla_tp(r):
Â  Â  Â  Â Â ifÂ r['TP1'] > r['TP4']:
Â  Â  Â  Â  Â  Â Â ifÂ r['GPU'].find('H20_3e')!=-1:
Â  Â  Â  Â  Â  Â  Â  Â Â returnÂ 'TP8'
Â  Â  Â  Â  Â  Â Â else:
Â  Â  Â  Â  Â  Â  Â  Â Â returnÂ 'TP4'
Â  Â  Â  Â Â else:
Â  Â  Â  Â  Â  Â Â returnÂ 'TP1'
Â  Â  Â  Â  Â  Â Â 
Â  Â Â defÂ mla_tp2(r):
Â  Â  Â  Â  tp = r['MLA_TP']
Â  Â  Â  Â Â returnÂ r[tp]

Â  Â Â #ä½¿ç”¨æœ€ä½³çš„TPç­–ç•¥ä¼°è®¡
Â  Â  df['MLA_TP'] = df.apply(lambdaÂ row: Â mla_tp(row),axis=1)
Â  Â  df['SparseMLA'] = df.apply(lambdaÂ row: Â mla_tp2(row),axis=1)
Â  Â Â 
Â  Â Â # ä¿®æ­£TPæ‰§è¡Œæ—¶é—´, æŒ‰ç…§åŠ è½½FP8çš„KVè®¡ç®—
Â  Â  df['DenseMLA'] = df['TP1'] + df['LoadKV_FP8']
Â  Â  df['SparseMLA'] = df['SparseMLA'] + df['LoadKV_FP8']

Â  Â  df['TPOT(Overlap)'] = (df['DenseMLA'] + df['DenseMLP']) * args.n_dense_layersÂ 
Â  Â  df['TPOT(Overlap)'] += (df['SparseMLA'] + df['SharedExpert'] + df['RoutedExpert']) * (args.n_layers - args.n_dense_layers)
Â  Â  df['TPOT'] = df['TPOT(Overlap)'] + (df['Dispatch'] + df['Combine']) * (args.n_layers - args.n_dense_layers)
Â  Â  df['GPU'] = df['GPU']+Â "("Â + df['MLA_TP'] +")"
Â  Â  df = df[['GPU','BatchSize','DenseMLA','DenseMLP','SparseMLA','Combine','SharedExpert','RoutedExpert','Dispatch','TPOT(Overlap)','TPOT']]
Â  Â  df['TPS_O'] =Â 1000Â / df['TPOT(Overlap)']
Â  Â  df['TPS'] =Â 1000Â / df['TPOT']
Â  Â  df['Total_O'] = Â df['TPS_O'] * df['BatchSize'].astype(int)
Â  Â  df['Total'] = Â df['TPS'] * df['BatchSize'].astype(int)
Â  Â  print(df.set_index('GPU').T.to_markdown(floatfmt=".3f"))
Â  Â Â returnÂ df
Â  Â Â 
dfs= decoding_time(args,gpu_decode,seq_len)


ç»Ÿè®¡ç»“æœå¦‚ä¸‹æ‰€ç¤º(å•ä½ms)



	
H800(TP1)
	
H800(TP1)
	
H800(TP1)
	
H800(TP4)
	
H20(TP4)
	
H20(TP4)
	
H20(TP4)
	
H20(TP4)
	
H20_3e(TP8)
	
H20_3e(TP8)
	
H20_3e(TP8)
	
H20_3e(TP8)


BatchSize
	
32.000
	
64.000
	
128.000
	
256.000
	
32.000
	
64.000
	
128.000
	
256.000
	
32.000
	
64.000
	
128.000
	
256.000


DenseMLA
	
0.135
	
0.270
	
0.540
	
1.081
	
0.753
	
1.506
	
3.011
	
6.023
	
0.745
	
1.490
	
2.979
	
5.959


DenseMLP
	
0.019
	
0.038
	
0.075
	
0.151
	
0.126
	
0.252
	
0.504
	
1.008
	
0.126
	
0.252
	
0.504
	
1.008


SparseMLA
	
0.135
	
0.270
	
0.540
	
0.781
	
0.384
	
0.592
	
1.008
	
1.840
	
0.285
	
0.395
	
0.613
	
1.050


Combine
	
0.082
	
0.165
	
0.329
	
0.659
	
0.082
	
0.165
	
0.329
	
0.659
	
0.082
	
0.165
	
0.329
	
0.659


SharedExpert
	
0.003
	
0.006
	
0.012
	
0.024
	
0.020
	
0.040
	
0.080
	
0.160
	
0.020
	
0.040
	
0.080
	
0.160


RoutedExpert
	
0.024
	
0.048
	
0.096
	
0.191
	
0.160
	
0.320
	
0.640
	
1.280
	
0.160
	
0.320
	
0.640
	
1.280


Dispatch
	
0.041
	
0.082
	
0.165
	
0.329
	
0.041
	
0.082
	
0.165
	
0.329
	
0.041
	
0.082
	
0.165
	
0.329


TPOT(Overlap)
	
9.858
	
19.716
	
39.431
	
61.497
	
35.367
	
60.511
	
110.800
	
211.379
	
29.613
	
49.005
	
87.787
	
165.351


TPOT
	
17.023
	
34.045
	
68.090
	
118.815
	
42.532
	
74.841
	
139.459
	
268.696
	
36.778
	
63.334
	
116.446
	
222.669


TPS_O
	
101.442
	
50.721
	
25.360
	
16.261
	
28.275
	
16.526
	
9.025
	
4.731
	
33.768
	
20.406
	
11.391
	
6.048


TPS
	
58.746
	
29.373
	
14.686
	
8.416
	
23.512
	
13.362
	
7.171
	
3.722
	
27.190
	
15.789
	
8.588
	
4.491


Total_O
	
3246.137
	
3246.137
	
3246.137
	
4162.779
	
904.803
	
1057.653
	
1155.230
	
1211.098
	
1080.591
	
1306.001
	
1458.077
	
1548.218


Total
	
1879.856
	
1879.856
	
1879.856
	
2154.610
	
752.383
	
855.149
	
917.831
	
952.749
	
870.082
	
1010.516
	
1099.225
	
1149.688

ä»¥TPS>20æ¡ä»¶è¿›è¡Œè¿‡æ»¤:

print(dfs[dfs['TPS_O']>20].set_index('GPU').T.to_markdown(floatfmt=".3f"))



	
H800(TP1)
	
H800(TP1)
	
H800(TP1)
	
H20(TP4)
	
H20_3e(TP8)
	
H20_3e(TP8)


BatchSize
	
32.000
	
64.000
	
128.000
	
32.000
	
32.000
	
64.000


DenseMLA
	
0.135
	
0.270
	
0.540
	
0.753
	
0.745
	
1.490


DenseMLP
	
0.019
	
0.038
	
0.075
	
0.126
	
0.126
	
0.252


SparseMLA
	
0.135
	
0.270
	
0.540
	
0.384
	
0.285
	
0.395


Combine
	
0.082
	
0.165
	
0.329
	
0.082
	
0.082
	
0.165


SharedExpert
	
0.003
	
0.006
	
0.012
	
0.020
	
0.020
	
0.040


RoutedExpert
	
0.024
	
0.048
	
0.096
	
0.160
	
0.160
	
0.320


Dispatch
	
0.041
	
0.082
	
0.165
	
0.041
	
0.041
	
0.082


TPOT(Overlap)
	
9.858
	
19.716
	
39.431
	
35.367
	
29.613
	
49.005


TPOT
	
17.023
	
34.045
	
68.090
	
42.532
	
36.778
	
63.334


TPS_O
	
101.442
	
50.721
	
25.360
	
28.275
	
33.768
	
20.406


TPS
	
58.746
	
29.373
	
14.686
	
23.512
	
27.190
	
15.789


Total_O
	
3246.137
	
3246.137
	
3246.137
	
904.803
	
1080.591
	
1306.001


Total
	
1879.856
	
1879.856
	
1879.856
	
752.383
	
870.082
	
1010.516

æ»¡è¶³ç”¨æˆ·TPS>20,åˆ™H800éœ€è¦BatchSize<=128, æ­¤æ—¶H800å³°å€¼æ¯å¡æ¯ç§’å¯ä»¥äº§ç”Ÿ3246ä¸ªtoken, è€ƒè™‘å®é™…çš„å³°è°·æ•ˆåº”å’Œä¸€äº›ä¸“å®¶è´Ÿè½½ä¸å‡è¡¡å¸¦æ¥çš„ä¸å¯Overlapçš„å»¶è¿Ÿ, çº¿ä¸Šç¯å¢ƒå¹³å‡æ¯å¡1850ä¸ªtoken/s.è¿™é‡Œä¹Ÿå¯ä»¥çœ‹åˆ°, åœ¨EP144æ—¶, batchsize=128æ— éœ€é€‰æ‹©TP=4æ‰§è¡ŒMLA, è€Œåœ¨å®˜æ–¹DeepSeek-V3è®ºæ–‡ä¸­,BatchSize=256æ—¶åˆ™ä½¿ç”¨TP=4æ—¶æ˜¯æœ€ä½³çš„, ä½†æ˜¯å®é™…ä¸Šè¿™ä¸ªè™½ç„¶å¯èƒ½æ˜¯ä¸€ä¸ªæ›´é«˜ååçš„æ–¹æ¡ˆ, ä½†æ˜¯TPSä¸æ»¡è¶³20, å› æ­¤å®˜æ–¹è¿˜æ˜¯é€‰æ‹©äº†EP144.

H20éœ€è¦ç»´æŒBatchSize<=32æ‰èƒ½æ»¡è¶³20TPSä»¥ä¸Šçš„éœ€æ±‚. æ­¤æ—¶H20æ€§èƒ½ä¸ºæ¥è¿‘900ä¸ªToken. H20_3eç”±äºæ›´å¤§çš„æ˜¾å­˜,å¯ä»¥é‡‡ç”¨TP=8å¹¶è¡Œåœ¨BatchSize<=64æ—¶ç»´æŒTPS> 20, æ­¤æ—¶çš„ååä¸º1306ä¸ªToken.

3.7 Overlapåˆ†æ

DeepSeekçš„å®˜æ–¹traceä¸­å¹¶æ²¡æœ‰decodeçš„ç›¸å…³çš„å†…å®¹, åªæ˜¯æœ‰ä¸€ä¸ªOverlapçš„å›¾.

ä»å‰ä¸€èŠ‚çš„æ±‡æ€»è¡¨å¯ä»¥çœ‹åˆ°, Combineæ˜¯æ˜¾è‘—å°äºAttentionçš„, å› æ­¤å®˜æ–¹å¯¹äºAttentionè¿›è¡Œäº†æ‹†åˆ†æ¥Overlap.æˆ‘ä»¬æŒ‰ç…§ä»¥ä¸‹è¯„ä¼°TimeBudget

dfo=dfs[dfs['TPS_O']>20]
dfo['TimeBudget'] = Â dfo['SparseMLA'] + dfo['SharedExpert'] - (dfo['Dispatch']+dfo['Combine'])
print(dfo[['GPU','BatchSize','TimeBudget']].set_index('GPU').to_markdown(floatfmt=".3f"))

GPU
	
BatchSize
	
TimeBudget


H800(TP1)
	
32
	
0.015


H800(TP1)
	
64
	
0.029


H800(TP1)
	
128
	
0.058


H20(TP4)
	
32
	
0.281


H20_3e(TP8)
	
32
	
0.182


H20_3e(TP8)
	
64
	
0.188
æ³¨:å…¶å®å¯¹äºH800çš„é€šä¿¡ä½™é‡æ˜¯å¾ˆå°çš„, å› æ­¤è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆéœ€è¦ä½¿ç”¨IBGDAçš„åŸå› .

å¯¹äºH20æˆ‘ä»¬æ³¨æ„åˆ°è¿˜æœ‰å¾ˆå¤§çš„æ—¶é—´é¢„ç®—, é‚£ä¹ˆæ˜¯å¦å¯ä»¥é€šè¿‡ä½¿ç”¨1.6T/800Gçš„å®ä¾‹å‘¢?è®¡ç®—å¦‚ä¸‹

h20_32 = GPU_perf( sm =Â 78Â ,comm_sm =Â 0,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 96,mem_bw =Â 3350,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 50,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)
h20_16 = GPU_perf( sm =Â 78Â ,comm_sm =Â 0,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 96,mem_bw =Â 3350,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 25,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)

h20_8 = GPU_perf( sm =Â 78Â ,comm_sm =Â 0,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 96,mem_bw =Â 3350,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 12.5,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)


h20_3e_32 = GPU_perf( sm =Â 78Â ,comm_sm =Â 0,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 141,mem_bw =Â 4800,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 50,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)

h20_3e_16 = GPU_perf( sm =Â 78Â ,comm_sm =Â 0,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 141,mem_bw =Â 4800,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 25,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)

h20_3e_8 = GPU_perf( sm =Â 78Â ,comm_sm =Â 0,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â fp16_flops =Â 118.4, fp8_flops =Â 236.8,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â mem =Â 141,mem_bw =Â 4800,
Â  Â  Â  Â  Â  Â  Â  Â  Â nvlink_bw =Â 400,pcie_bw =Â 12.5,
Â  Â  Â  Â  Â  Â  Â  Â  Â discount_rate =Â 0.85)


gpu_decode_h20 = dict({'H20-3.2T': h20_32,'H20-1.6T': h20_16,'H20-800G': h20_8,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'H20_3e-3.2T': h20_3e_32,'H20_3e-1.6T': h20_3e_16,'H20_3e-800G': h20_3e_8,})

dfs= decoding_time(args,gpu_decode_h20,seq_len)
dfo=dfs[dfs['TPS_O']>20]
dfo['TimeBudget'] = Â dfo['SparseMLA'] + dfo['SharedExpert'] - (dfo['Dispatch']+dfo['Combine'])
print(dfo[['GPU','BatchSize','TimeBudget']].set_index('GPU').to_markdown(floatfmt=".3f"))

GPU
	
BatchSize
	
TimeBudget


H20-3.2T(TP4)
	
32
	
0.281


H20-1.6T(TP4)
	
32
	
0.157


H20-800G(TP4)
	
32
	
-0.090


H20_3e-3.2T(TP8)
	
32
	
0.182


H20_3e-3.2T(TP8)
	
64
	
0.188


H20_3e-1.6T(TP8)
	
32
	
0.058


H20_3e-1.6T(TP8)
	
64
	
-0.059


H20_3e-800G(TP8)
	
32
	
-0.189


H20_3e-800G(TP8)
	
64
	
-0.553

ç»“è®ºä¸º800Gå®ä¾‹æ— æ³•æ»¡è¶³éœ€æ±‚, 1.6Tå®ä¾‹ä»ç„¶æœ‰å¾ˆå¤§çš„é€šä¿¡ä½™é‡. ä½†æ˜¯å¯¹äºH20_3eçš„å®ä¾‹è¿˜æ˜¯éœ€è¦é…ç½®3.2Tçš„ç½‘ç»œ, ä»¥ä¿è¯æ›´å¤§çš„BatchSizeä¸‹çš„æ€§èƒ½, ä½†æ˜¯è¿™é‡Œåˆå­˜åœ¨ä¸€ä¸ªæˆæœ¬æ ¸ç®—çš„é—®é¢˜.å¦å¤–ä»timebudgetçš„è§’åº¦æ¥çœ‹, ç½‘å¡ç­‰é™æ€å»¶è¿Ÿçš„å½±å“å¯ä»¥å¿½ç•¥ä¸è®¡.

4. å°ç»“

æœ¬æ–‡é€šè¿‡å¯¹è®¡ç®—é‡/å†…å­˜å¸¦å®½/ç½‘ç»œå¸¦å®½ç­‰å‡ æ–¹é¢çš„çº¦æŸ,è¯¦ç»†çš„é€†å‘åˆ†æäº†DeepSeek-R1åœ¨H800å’ŒH20ä¸Šçš„æ€§èƒ½. H800æœ€ä½³éƒ¨ç½²å³ä¸ºå®˜æ–¹çš„EP144æ–¹æ¡ˆ, åˆ†ææ•°æ®å’Œå®˜æ–¹æ•°æ®åŸºæœ¬ä¸€è‡´. å¦å¤–å¯¹äºH800çš„Overlapæ—¶é—´é¢„ç®—æ¥åˆ†æ, å¿…é¡»è¦ä½¿ç”¨IBGDAæ¥é™ä½å»¶è¿Ÿ.

è€ŒH20éƒ¨ç½²ä¸­, æˆ‘ä»¬å‘ç°ç”±äºç®—åŠ›çš„çº¦æŸä½¿å¾—MLAè®¡ç®—ç¼“æ…¢éœ€è¦é€šè¿‡TPå¹¶è¡ŒåŠ é€Ÿ, ä½†æ˜¯TPè¿‡å¤§æ—¶åˆä¼šå› ä¸ºå¤§é‡çš„KVCacheçš„å ç”¨å¯¼è‡´batchsizeå—é™. æ­¤æ—¶H20-3E(141GB)çš„ç‰ˆæœ¬æ˜¾ç¤ºå‡ºäº†é¢å¤–çš„æ€§èƒ½æ”¶ç›Š. å¦å¤–æˆ‘ä»¬è¿˜å¯¹H20çš„äº’è”å¸¦å®½è¿›è¡Œäº†è¯„ä¼°, åœ¨EPå¹¶è¡Œå®ç°æ°å½“æ—¶, 1.6Tbpså¸¦å®½å³å¯æ»¡è¶³éœ€æ±‚.

å‚è€ƒèµ„æ–™
[1]Â 

DeepSeek-V3 / R1 æ¨ç†ç³»ç»Ÿæ¦‚è§ˆ:Â https://zhuanlan.zhihu.com/p/27181462601

[2]Â 

DeepSeek V3/R1 æ¨ç†æ•ˆç‡åˆ†æï¼ˆ2ï¼‰: DeepSeek æ»¡è¡€ç‰ˆé€†å‘å·¥ç¨‹åˆ†æ:Â https://zhuanlan.zhihu.com/p/29841050824



å¤§æ¨¡å‹æ¶æ„
37
äº‘åŸºç¡€è®¾æ–½
74
AIåŠ é€Ÿå™¨äº’è”
39
å¤§æ¨¡å‹æ¶æ„ Â· ç›®å½•
ä¸Šä¸€ç¯‡
ä»DeepSeek MoEä¸“å®¶è´Ÿè½½å‡è¡¡è°ˆèµ·
â€‹

Scan to Follow
