Title: Gaussians

URL Source: https://gestalt.ink/gaussians

Markdown Content:
Understanding Gaussians
-----------------------

The **Gaussian distribution**, or **normal distribution** is a key subject in statistics, machine learning, physics, and pretty much any other field that deals with data and probability. Itâ€™s one of those subjects, like Ï€ or Bayesâ€™ rule, that is so fundamental that people treat it like an icon.

*   [Understanding Gaussians](https://gestalt.ink/gaussians#understanding-gaussians)
    *   [Building a geometric intuition for Gaussians](https://gestalt.ink/gaussians#building-a-geometric-intuition-for-gaussians)
        *   [The standard Gaussians](https://gestalt.ink/gaussians#the-standard-gaussians)
        *   [A family of Gaussians](https://gestalt.ink/gaussians#a-family-of-gaussians)
        *   [Spherical, diagonal and degenerate Gaussians](https://gestalt.ink/gaussians#spherical-diagonal-and-degenerate-gaussians)
        *   [Means](https://gestalt.ink/gaussians#means)
        *   [(Co)variances](https://gestalt.ink/gaussians#covariances)
            *   [The variance of N1s](https://gestalt.ink/gaussians#the-variance-of-n1_s)
            *   [The covariance](https://gestalt.ink/gaussians#the-covariance)
            *   [Of spherical, diagonal and degenerate Gaussians](https://gestalt.ink/gaussians#of-spherical-diagonal-and-degenerate-gaussians)
    *   [Fundamental properties of Gaussians](https://gestalt.ink/gaussians#fundamental-properties-of-gaussians)
        *   [Linear transformations](https://gestalt.ink/gaussians#linear-transformations)
        *   [If you can linearly transform it to a Gaussian, itâ€™s a Gaussian](https://gestalt.ink/gaussians#if-you-can-linearly-transform-it-to-a-gaussian-its-a-gaussian)
        *   [There is always an _invertible_ transformation](https://gestalt.ink/gaussians#there-is-always-an-invertible-transformation)
        *   [The sum of two Gaussians is a Gaussian](https://gestalt.ink/gaussians#the-sum-of-two-gaussians-is-a-gaussian)
        *   [Chaining Gaussians](https://gestalt.ink/gaussians#chaining-gaussians)
        *   [Conditioning Gaussians](https://gestalt.ink/gaussians#conditioning-gaussians)
    *   [Deriving the density](https://gestalt.ink/gaussians#deriving-the-density)
    *   [Sources and other materials](https://gestalt.ink/gaussians#sources-and-other-materials)
    *   [References](https://gestalt.ink/gaussians#references)

To start at the beginning: the normal distribution is a _probability distribution_: a mathematical object that describes a process by which you can _sample data_. Here is an example. If I measure the height of about 2000 female soldiers in the US army, and plot the results in a histogram, here is what that might look like.

![Image 1](assets/e/1/e151a3de36ce62750136c1982c9c840b.svg)

The _stature_ (height) of 1986 female soldiers in the US Army. From the ANSUR II dataset \[[1](https://gestalt.ink/gaussians#references)\].

You can see that the data is clustered around the _mean value_. Another way of saying this is that the distribution has a definite _scale_. That is, even though people can have all sorts of heights, there are clear limits. You might see somebody who is 1 meter taller than the mean, and it might theoretically be possible to be 2 meters taller than the mean, but thatâ€™s it. People will never be 3 or 4 meters taller than the mean, no matter how many people you see.

The definite scale of the height distribution is why we can have doors. We know that heights will fall in a certain range, so we can build for that. There are a few distributions like this with a definite scale, but the Gaussian is the most famous one. You can see in the plot above that it has a kind of â€œbellâ€ shapeâ€”itâ€™s also called _the bell curve_â€”which trails off smoothly as we get further from the mean, first slowly and then dropping rapidly, and then flattening out quickly. If we make the bins in our histogram smaller, and increase the sample size so they are still filled up, we can see the shape appear more clearly.

![Image 2](assets/1/8/1874b128c98cbca9ae68f7a0edcc40f9.svg)

Synthetic data for 100 million imagined soldiers from the same distribution as the figure above.

If you measure more than one thing about your subject, you get _multivariate_ data, and the resulting distribution is called a _multivariate distribution_. For example, if we take our soldiers, and measure their height and their weight, the data looks like this.

![Image 3](assets/4/6/46f887c735f27c1df0a169557227fb93.svg)

A scatter plot of the height and weight of our sample of soldiers.

This is called _a multivariate normal distribution_. Like the one-dimensional (_uni_variate) version, the data is clustered around a central value.

Most descriptions you will read of the Gaussian distribution will focus on the way it is used to describe or approximate the real world: its use as _a model_. This is a typical statistics approach, and it comes with a lot of baggage that we will not discuss here.

In this article, we want to focus more on the way Gaussians are used in _machine learning_. There, we also aim to build a model of our data, but we are often less concerned with the fact of capturing our data in a single Gaussian. Instead, we use Gaussians as a _building block_, a small part of a more complex model. For instance, we might add noise from a Gaussian to our data at some point in our algorithm, or we could have a neural network produce the parameters of a Gaussian as part of its prediction. We could combine multiple Gaussians together, in order to create a distribution with multiple peaks. We could even take a sample from a Gaussian and feed it to a neural net, so that the neural net effectively twists and folds the relatively simple shape of the Gaussian into something much more complex.

![Image 4](assets/4/f/4fbf09197570ff45fea874fea69c44f7.png)

100 000 points drawn from a Gaussian distribution and passed through a randomly initialized neural network.

To use Gaussians in this way requires a solid intuition for how they behave. The best way to do that, I think, is to do away entirely with the symbolic and mathematical foundations, and to derive what Gaussians are, and all their fundamental properties from purely geometric and visual principles. Thatâ€™s what weâ€™ll do in this article.

Building a geometric intuition for Gaussians
--------------------------------------------

If youâ€™re not intimately familiar with Gaussians, you would be forgiven for thinking of them as one of the most monstrously complicated probability distributions around. After all, when we learn about them, pretty much the first thing we see is their probability density function.

N(ğ±âˆ£Î¼,ğšº)\=1(2Ï€)k|ğšº|âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆšexp(âˆ’12(ğ±âˆ’Î¼)Tğšºâˆ’1(ğ±âˆ’Î¼))

Itâ€™s a beast of a formula, especially if youâ€™re not used to reading such things. Why then, do people like this distribution so much? Why use it as a building block when it is already so complex? Shouldnâ€™t we look for simple building blocksâ€”perhaps something like a uniform distribution, which has a much simpler formula?

Partly, we like the Gaussian because it has nice properties, but partly, we like it because once you get to know it, itâ€™s not so complicated. You just have to let yourself forget about the complicated formula. So weâ€™ll put it out of our minds, and start elsewhere.

The plan is as follows. We will first derive a _standard_ Gaussian. Just one distribution, in one dimension. This makes the formula much simpler. From this, we will define a standard Gaussian in n dimensions, in a straightforward way, which doesnâ€™t require us to extend the formula very much. Then, we will use _affine transformations_â€”multiplication by a matrix, and addition of a vectorâ€”to define a whole _family_ of Gaussians. Implicitly, this will lead to the formula we use above, but practically, all we need to understand are the basic rules of linear algebra.

We will use this view to derive a bunch of useful properties about Gaussians, and then finally wrap up by showing that the above formula is indeed correct.

### The standard Gaussians

The first thing we need is the standard Gaussian in one dimension. This is a probability distribution on the real number line: if we sample from it, we can get any real number. The function that describes it is a _probability density function_: it maps each real number to a probability density. Numbers with higher density are in some sense more likely than numbers with low density.

To come up with the density function, remember the aim we started with: we want the distribution to have a definite scale, some area where almost all of the probability mass is concentrated. We'll put that region around zero on the number line (it seems as good a point as any). This is where the density should peak, and as we move away from zero the density should drop very quickly, so that pretty soon, it's almost zero. One way of achieving this is to have _exponential decay_. Just like an exponential function ex blows up very quickly, the negative exponential function eâˆ’x drops to zero extremely quickly.

We could use the exponential function, but if we add a square in there, to give us eâˆ’x2 we get some nice properties on top of the exponential decay.

![Image 5](assets/2/7/273a4b320229af311228c387b1ade6d2.svg)

First of all, the decay far out from zero is even faster, since weâ€™re adding in a square. Second, close to zero, we get a little more probability density on all numbers in that region. The exponential decay really favors only 0, while the squared exponential favours all numbers _near zero_. Finally, the quared exponential has two _inflection points_, highlighted in the image with diamonds. These are the points where the decay moves from dropping faster and faster to dropping slower and slower. These inflection points form a nice, natural marking for the _scale_ of the distribution: we can take the interval between the inflection points as the â€œtypicalâ€ range of outcomes that we might get if we sample from the distribution. The numbers outside this range are possible, but theyâ€™re less likely.

So thatâ€™s where the basic bell shape of the distribution comes from: the choice to have the probability density decay squared exponentially. Next, weâ€™ll make a small adjustment to make the function a little more well-behaved. Remember that the inflection points give a us a nice interval to consider the â€œtypical pointsâ€. This interval is now a little arbitrary. If we scale the function a bit, we can put the inflection points at âˆ’1 and 1, so that the interval containing the bulk of the probaility mass is contained in (âˆ’1,1). This seems like a nice property to have, and as it turns out, it doesnâ€™t make the function much more complex.

First, we need to figure out where the inflection points are. We defined them as the point where the function moves from dropping faster and faster to dropping slower and slower. This behavior, how fast the change in the function changes, is given by the second derivative of the function. Where that is equal to zero, we find an inflection point. The first derivative of eâˆ’x2 is (using the chain rule) âˆ’2xeâˆ’x2, and the second derivative is (using the product rule) âˆ’2eâˆ’x2+4x2eâˆ’x2\=(4x2âˆ’2)eâˆ’x2. Setting that equal to zero, we get x2\=1/2, so the inflection points are at

x\=âˆ’12âˆ’âˆ’âˆšandx\=12âˆ’âˆ’âˆš.

If we want to stretch a function f(x) vertically by a factor of y, we should multiply its input by 1/y: f(1yx). That means that if we want to stretch it so that the point x ends up at 1â€”a stretch of 1/xâ€”we should multiply the input by x

In our case, that means we multiply the input by 12âˆ’âˆ’âˆš:

eâˆ’(12âˆšx)2\=eâˆ’12x

![Image 6](assets/8/e/8e73fcef6c397467292582299a54056a.svg)

So, our function is now eâˆ’12x2. The extra multiplier of 12 is a small price to pay to put the inflection points at âˆ’1 and 1.

With that, we almost have a probability density function. The only problem left is that the rules of probability density functions state that the whole area under the curve should integrate to 1. Put simply, the probability of sampling any number in (âˆ’âˆ,âˆ) should be 1.

We could check whether it does, and if it doesnâ€™t, we could stretch or squeeze the function vertically until it does. This would require some complicated analysis, which, while fun, is exactly the kind of thing we are trying to avoid. To keep things simple, we will simply assume that the area under the whole curve, from negative to positive infinity, is some finite value.

Whatever the area under the curve eâˆ’12x2 is, we will call that z. By the rules of integration, multiplying our function by 1/z will then yield a function that integrates to 1. Since z is a constant, we can say that the scaled function, which we will call Ns, is _proportional to_ the unscaled function:

Ns(x)âˆeâˆ’12x2.

That is, Ns, which is a proper probability density, is a bit more complicated than eâˆ’12x2 but all that complexity is in some multiplicative constant. One other trick we can use to simplify things is to focus on the logarithm of the probability density. In our case, we get:

lnÂ Ns(x)\=+âˆ’12x2

where the symbol \=+ means that both sides are equal except for some term (âˆ’lnz in this case) that doesnâ€™t depend on x.

With that, we have defined our _standard Gaussian_ in one dimension as precisely as we need. We donâ€™t have the complete functional form of the density,but we donâ€™t need it. We know the function exists, and we know what it looks like. We can now derive the full family of Gaussians.

First, to make the leap to _multivariate_ Gaussians, we define a single multivariate _standard_ Gaussian. In n dimensions, we will call this distribution Nns. Itâ€™s a distribution over vectors ğ± of n elements.

We define Nns by a _sampling process_. To sample from the multivariate Gaussian in n dimensions, we sample n separate values x1 though xn from the standard one-dimensional Gaussian Ns (which weâ€™ve just defined) and we concatenate them into a vector ğ±. To say that the random vector ğ± is _distributed according to the standard Gaussian_ Nns in n dimensions, we write ğ±âˆ¼Nns. This means that

â›ââœx1â‹®xnââ âŸwithxiâˆ¼Ns.

If ğ± is distributed according to Nns, then each individual element of ğ± is distributed according to Ns.

This is a complete definition of the standard Gaussian. We havenâ€™t defined a density function for Nns, but weâ€™ve defined how to sample from it, which is all we need for a definition. The density function exists _implicitly_.

We can now ask ourselves what this density function looks like. We can derive the general form very easily from one basic property: that of _independence_. Since we sample the elements of ğ± independentlyâ€”how we sample one does not depend on how we sample the othersâ€”the probability density of the whole vector is the probability density of the elements multiplied together:

Nns(ğ±)\=p(x1)â‹…p(x2)â‹…â€¦â‹…p(xn)

Now, switching to log-probability densities, we can get a sense of the shape of the function. Remember that \=+ means equal up to some constant term, so we can remove any terms that donâ€™t depend on elements of ğ±.

lnÂ Nn(ğ±)\=lnÂ p(x1)+â€¦+lnÂ p(x)\=lnÂ Ns(x1)+â€¦+lnÂ Nsp(x)\=+âˆ’12x12âˆ’â€¦âˆ’12xn2\=âˆ’12(x12+â€¦+xn2)\=âˆ’12â€–ğ±â€–2

The last line follows from recognizing that the right hand side has become equal to the vector norm without the square root. That is, the square of the norm: â€–ğ±â€–2\=x12+â€¦+xn2. Taking the logarithm away again, we get

Nns(ğ±)âˆeâˆ’12â€–ğ±â€–2.

That is, the probability density at any point ğ± depends only on the norm of ğ±â€”how far away from ğŸ we are. Imagining this in two dimensions to start with, this tells us that all points with the same distance to ğŸ, any set of points that forms a _circle_, have the same density. The function also tells us that as the norms (and thus the circles) get bigger, the probability density of the points in that circle decays in the same way as the density decays in Ns: according to a negative squared exponential.

With that, we have a pretty clear picture of what the standard multivariate Gaussian looks like. Itâ€™s rotationally symmetric, since all circles have the same density, and it decays in the same way as the bell shape of Ns. Putting this together, tells us that it should look, in two dimensions, like the function of Ns rotated about the origin.

![Image 7](assets/8/b/8b6106ae65103ba0b060db785b54057b.svg)

In two dimensions, the set of all points that have the same densityâ€”like one of the the circles in the picture aboveâ€”is called a _contour line_. The standard Gaussian is called a _spherical_ distribution because all its contour lines are circles (two-dimensional spheres). In higher dimensions, where things are more difficult to visualize, the same principle holds: the density of ğ± under Nns depens only on the norm of ğ±, so the set of all points with the same density is the set of all points with the same norm, a (hyper)-sphere. These spheres are called the _contour surfaces_ of Nns. The principle of contour surfaces will be very helpful going forward, in building up an intuition for what general Gaussians look like.

Moving forward, we will drop the superscript from Nns when the dimensionality is clear from context. Likewise, we will use N1s to emphasize that we are talking about the one-dimensional Gaussian if necessary.

### A family of Gaussians

Next, letâ€™s build the rest of the family. We do this by taking the standard Gaussian Ns in n dimensions, and _transforming_ it linearly. We will start, again, with a sampling process.

We sample an n\-dimensional vector ğ¬ from Ns and apply any linear operation ğ±\=ğ€ğ¬+ğ­ with a matrix ğ€âˆˆâ„mÃ—n and ğ­âˆˆâ„m. This results in a random vector ğ±, since part of this process (the sampling of ğ¬) is random.

Now, we _define_ a Gaussian to be any distribution that results from this process, for some choice of ğ€ and ğ­. We will, refer to such a Gaussian as N(ğ€,ğ­).

We have defined how to sample a point from N(ğ€,ğ­), so we have fully defined this Gaussian. Obviously, it would be interesting to know what the resulting density function looks like, but that doesnâ€™t need to be _its definition_. We can work that out from how we defined the sampling process. Weâ€™ll try to do that, and to work out some properties of the distribution we have now defined, without getting into the complicated formula for the density function.

For the time being, assume that ğ€ is square and invertible, so that no two points are mapped to the same point by ğ€.

To help us understand the shape of the density function, we can think back to the contour circles we defined for Ns, let's say the one for â€–ğ±â€–\=1. Each of the points ğ± in this circle could be sampled from Ns and transformed by ğ€ and ğ­. What happens to a circle when all its points are transformed by a matrix? It becomes an _ellipse_. What's more, the relative lengths of vectors are maintained under matrix multiplicationâ€”if â€–ğšâ€–<â€–ğ›â€– then â€–ğ€ğšâ€–<â€–ğ€ğ›â€–â€”so any point inside the circle (any point with â€–ğ±â€–<1) before the transformation is inside the _ellipse_ after the transformation. Any point outside the circle before, is outside the ellipse after.

![Image 8](assets/f/7/f7ca8ee3e59a398279623bc7ec492cd4.svg)

Transforming the standard Gaussian by an affine transformation turns the contour circles into contour ellipses. Any point inside one of the circles before the transformation will be inside the corresponding ellipse after.

This means that the amount of probability mass captured inside the unit circle before the transformation, is captured inside the corresponding ellipse after the transformation. After all, when we are sampling, these are the same points: if p is the probability of sampling some ğ¬ inside the circle before the transformation, then that is the probability of sampling some ğ± inside the corresponding ellipse.

For higher dimensions, the circles becomes hyper-spheres and the ellipses become ellipsoids, but the basic intuition stays the same.

If ğ€ is not square and invertible, the picture is a little more complex. If, for example ğ¬ is three-dimensional and ğ± is two-dimensional, then we are taking all points ğ¬ on a sphere, and projecting them down to two dimensions. The result is still an ellipse in two dimensions, but not all points are on the edge of the ellipse anymore. Some are in the interior. This means we no longer have the property that if â€–ğšâ€–<â€–bâ€– then â€–ğ€ğšâ€–<â€–ğ€ğ›â€–. However, we will be able to show in a bit that this distribution is equivalent to one defined with a two-dimensional ğ¬ and a square, invertible ğ€. Thus, this messiness isn't really any cause for concern. We can still call this a Gaussian, and think of it as being mapped from Ns in a neat way that maps contour circles to contour ellipses.

![Image 9](assets/4/d/4d0de4346dd374ec372c281e63a1e29e.png)

If the transformation is from three to two dimensions, points on the sphere (left) may end up inside the corresponding ellipse (right).

### Spherical, diagonal and degenerate Gaussians

Before we move on, it pays to investigate what kind of family members this family of ours has. Weâ€™ll look at three special types of Gaussians: spherical, diagonal and degenerate.

The simplest type of Gaussian is the **spherical Gaussian**, also known as an _isotropic_ Gaussian. This is the special case when the contour surfaces, which are spheres before the transformation, are still spheres after the transformation.

This happens only when we expand ğ¬ uniformly in all directions. Or, in other words, when we multiply it by a scalar. That is, if

ğ±\=Ïƒğ¬+ğ­

for some scalar Ïƒ, then the distribution on ğ± is a sperical Gaussian. To fit this into the standard affine transformation framework, we can insert an identity matrix and get:

ğ±\=Ïƒğˆğ¬+ğ­.

This shows that the matrix Ïƒğˆâ€”a diagonal matrix with Ïƒ at every point on the diagonalâ€”is the matrix we should multiply by to get a spherical Gaussian.

The spherical Gaussians are particularly simple, and using them will simplify many aspects of the use of Gaussians. In machine learning, you will see them used in, for example, diffusion models.

A class that allows for a bit more variation is the **diagonal Gaussian**. Here, we again use a diagonal matrix in our affine transformation, but we let the diagonal values vary. That is, we define some _vector_ Ïƒ, and we [place these values along the diagonal of a matrix](https://gestalt.ink/diagonal-matrix). That matrix then becomes our transformation matrix.

A diagonal ğƒ matrix represents a particularly simple transformation. The dimension i is simply multiplied by the value Dii, ignoring whatever happens elsewhere in the matrix.

Visually, the result is that the circles or spheres in the standard normal distribution are stretched into ellipses, but _only along the axes_. Any ellipse is allowed, but the major axis of the ellipse (the line from tip to tip) has to point along one of the axes of our coordinate system.

Practically, this means that the distribution has zero _correlation_ between the elements of ğ±. If I tell you the value of x1, it carries no information about the value of x2. Study the example of the heights and widths of the soldiers above to see the opposite case: the points are roughly on a diagonal line, so if I tell you that a particular soldier has a certain height, you can make an informed guess about what their weight is likely to be. For that sort of reasoning, you need more than a diagonal Gaussian. You get this by playing non-zero values on the off-diagonal elements of your transformation matrix.

The final special case we will discuss is the **degenerate Gaussian**. This is what happens when, for example, we map a one-dimensional ğ¬ to a two-dimensional ğ±.

![Image 10](assets/6/8/6878c8c7916a28c43d6d877ae2db4d3b.svg)

Since all points ğ¬ lie on a line, the resulting points ğ± can only lie on a line, even though theyâ€™re in a two-dimensional space. Weâ€™ve decided to call this a Gaussian, and along the line, you will see the familiar bell shape, but itâ€™s fundamentally different from a true two-dimensional Gaussian like N2s, that fills all of â„2.

We call this a degenerate Gaussian. If ğ± has d dimensions, the _support_ of the Gaussian, the set of all points that have non-zero density isnâ€™t the whole of â„d, in fact, itâ€™s a linear subset of it (a hyperplane).

Within the support, the distribution looks like a normal Gaussian: a bell shape and non-zero probability density everywhere, decaying squared-exponentiallyas we move away from the mean. We call a Gaussian that does have a non-zero density everywhere a _non-degenerate_ Gaussian, or a _Gaussian with full support_.

### Means

Next, letâ€™s look at the properties of these Gaussians. First, the mean ğ±Â¯. If we average a bunch of samples from Ns and let the number of samples go to infinity, where do we end up? This is called the [expected value](https://gestalt.ink/expectation): ğ±Â¯\=Eğ±âˆ¼Nsğ±

The definition of the expected value for continuous functions like these involves an integral, but happily, we donâ€™t need to open it up. We just need to remember some key properties:

*   Expectation distributes over (vector) sums. That is Eğ±(f(ğ±)+g(ğ±))\=Eğ±f(ğ±)+Eğ±g(ğ±).
*   If we have (matrix) multiplication or (vector) addition inside the expectation, we can move it outside. That is Eğ±ğ­+ğ€ğ±\=ğ­+ğ€Eğ±ğ±.

Letâ€™s start with the mean of the standard Gaussian. You may be able to guess what this should come out to. The density peaks at ğŸ, and the function is radially symmetric around ğŸ. If we think of the mean as the center of mass of the density function, there isnâ€™t really any other point that could qualify.

Itâ€™s relatively simple to show that this guess is correct, because the components of ğ± are independently drawn. If we ask for the i\-th element of the mean, we need only look at the i\-th elements of our samples. These are all samples from N1s, so the mean for that component is the mean of N1s. N1s is symmetric around 0, so its mean must be 0. In short, the mean for the standard Gaussian is the zero vector.

What about the mean of our transformed Gaussian N(ğ€,ğ­)? If we use the basic poperty that the expectation is a linear functionâ€“that is, we can move additions and multiplications outside the expectation)â€”we can show very simply that the mean is equal to the translation vector in our transformation, ğ­:

Eğ±âˆ¼N(ğ€,ğ­)ğ±\=Eğ¬âˆ¼Nsğ­+ğ€ğ¬\=ğ­+ğ€Eğ¬ğ±\=ğ­+ğŸ.

### (Co)variances

#### The variance of N1s

The variance is a measure of how widely the sampled points are spread about the mean. Weâ€™ll need to work out the variance of N1s first. It is defined as the expected value of the squared distance to the mean: E(xâˆ’xÂ¯)2. Since the mean is 0, we are just looking for the expected value of x2.

We could solve this by unpacking this expectation into its integral definition and working it out, but that requires a lot of heavy math. Happily, thereâ€™s a nifty trick that allows us to minimize the amount of time we need to spend in integral-land. First, letâ€™s see what the integral is that weâ€™re looking for. To make things easier to follow, Iâ€™ll give away that the answer is 1, and weâ€™ll work towards that.

First, weâ€™ll call the unscaled density function f. That is

f(x)\=eâˆ’12xÂ andÂ p(x)\=1zf(x).

Then, the variance is

Ex2\=âˆ«âˆâˆ’âˆp(x)x2dx\=1zâˆ«âˆâˆ’âˆx2f(x)dx.(var)

To show that the variance of the standard normal distribution is 1, we need to show that the integral marked in green at the end is equal to z (which is the name we gave to the area under the curve of f). This is where we can use a trick.

The trick requires us to take the second derivative of f. We have

fâ€²(x)fâ€³(x)\=âˆ’xeâˆ’12x2\=(x2âˆ’1)eâˆ’12x2\=x2f(x)âˆ’f(x).

Note that the function weâ€™re taking the integral for x2f(x) has popped up on the right-hand-side. If we re-arrange this last line, we see

x2f(x)\=fâ€³(x)+f(x).

Filling this into the integral, we get

âˆ«âˆâˆ’âˆx2f(x)dx\=âˆ«âˆâˆ’âˆfâ€³(x)+f(x)dx\=âˆ«âˆâˆ’âˆfâ€³(x)dx+âˆ«âˆâˆ’âˆf(x)dx.

Now, the first term, âˆ«âˆâˆ’âˆfâ€³(x), is equal to 0. We solve it by taking the antiderivative fâ€²(x) and working out fâ€²(âˆ)âˆ’fâ€²(âˆ’âˆ). The derivative of f is 0 at both ends, since the function flattens out towards infinity, so the answer is 0âˆ’0\=0.

That leaves us with the second term, âˆ«âˆâˆ’âˆf(x)dx, which is exactly the definition of z. So we have worked out that

âˆ«âˆâˆ’âˆx2f(x)dx\=z

which, if we fill it in aboveâ€”in equation (var)â€” shows that the variance of N1s is 1.

Now that we know what the variance of the standard, one-dimensional Gaussian is, the hard work is done. The parameters of the rest of the Gaussians follow straightforwardly.

#### The covariance

For a multivariate distribution on a vector ğ± there are many variances to capture. There is first the variance along each dimension xi, but also the _co_variance of every element xi with every other element xj.

The [covariance matrix](https://gestalt.ink/covariance-matrix) ğšº captures all of this. For a random vector ğ± this is defined as the expected outer product of the deviation from the mean E(ğ±Â¯âˆ’ğ±)(ğ±Â¯âˆ’ğ±)T. This is a square matrix. It contains all the variances of the individual elements xi of ğ± along its diagonal, and it contains all the covariances between elements xi and xj on its off-diagonal elements.

Letâ€™s start with the covariance matrix of the standard Gaussian Ns. We know that in ğšº, the diagonal elements are the variances of xi. These are 1, because we sampled them independently from N1, which has variance 1. The off-diagonal elements are the co-variances between any two of the elements xi and xj. We know these are 0, because we sampled each xi independently. So, in a phrase, the covariance matrix of Ns is the identity matrix ğˆ.

Now for the rest of the Gaussians. The covariance matrix of ğ±\=ğ€ğ¬+ğ­ is defined as the expected outer product of the vector ğ±âˆ’ğ±Â¯, where ğ±Â¯ is the mean of ğ±. We already know that ğ±Â¯\=ğ­, so we are looking for the expected outer product of ğ±âˆ’ğ±Â¯\=ğ€ğ¬+ğ­âˆ’ğ­. This gives us.

Eğ±âˆ¼N(ğ€,ğ­)(ğ±âˆ’ğ±Â¯)(ğ±âˆ’ğ±Â¯)T\=Eğ¬âˆ¼Ns(ğ€ğ¬)(ğ€ğ¬)T\=Eğ€ğ¬ğ¬Tğ€T\=ğ€(Eğ¬ğ¬T)ğ€T\=ğ€ğˆğ€T\=ğ€ğ€T.

Note that in the second line we are again using the fact that the expectation is a linear function, so we can take matrix multiplications outside of the expectation (on the left and on the right).

So, to summarize, if we build our Gaussian by transforming Ns with a transformation matrix ğ€ and a translation vector ğ­, we end up with a distribution with mean ğ­ and covariance matrix ğšº\=ğ€ğ€T.

We can now make the leap from _properties_ to _parameters_. Instead of identifying a particular Gaussian by the transformation ğ€,ğ­ we used to create it, we can identify it by the covariance ğšº and mean ğ­ of the resulting distribution.

The Gaussian we get from the transformation ğ€ğ±+ğ­ on the standard normal distribution is called N(Î¼,ğšº), with Î¼\=ğ­ and ğšº\=ğ€ğ€T.

This also means that Ns\=N(ğŸ,ğˆ), which is how weâ€™ll refer to it from now on.

#### Of spherical, diagonal and degenerate Gaussians

Itâ€™s worth thinking briefly about what the covariance matrix looks like for the three special categories of Gaussian that we discussed earlier: spherical, diagonal and degenerate.

For the spherical and the diagonal Gaussian, remember that ğ€ is a diagonal matrix. This means that the covariance matrix ğ€ğ€T is equal to ğ€ğ€, since ğ€ is symmetric, so ğ€T\=ğ€. The product of two diagonal matrices is very simple: it is another diagonal matrix, with at each point along the diagonal, the product of the corresponding elements of the two matrices.

The result is that for a spherical Gaussian with standard deviation Ïƒ, while ğ€ is a diagonal matrix with Ïƒ along the diagonal, the covariance matrix is a diagonal matrix with Ïƒ2 along the diagonal. This is of course, the variance.

Likewise for the diagonal Gaussian, we have the standard deviations along the diagonal of ğ€, and their squares, the variances, along the diagonal of the covariance ğ€ğ€T.

In both cases, the covariances (the off-diagonal elements of ğ€Tğ€) are zero. This shows that there is no correlation between the axes: if our Gaussian is diagonal, we cannot predict the value of one dimension from one of the other dimensions.

Finally, letâ€™s look at the degenerate Gaussians. We get a degenerate Gaussian if ğ€â€™s _[rank](https://gestalt.ink/rank)_ is less than the output dimension. Or, put differently, for ğ±âˆˆâ„d, if it has fewer than d linearly independent columns. If this happensâ€”say there are k linearly independent columns in ğ€ and the rest can be expressed as a linear combination of these k columnsâ€”then any ğ¬ multiplied by ğ€ is mapped to a space of dimension k, since the multiplication is a linear combination of k vectors.

We can get some insight into the consequences by looking at the [singular value decomposition](https://gestalt.ink/svd) (SVD) of ğ€.

Let ğ€\=ğ”ğšºğ•T be the full SVD of ğ€. If ğ€ maps its input into an output of dimension k, then the diagonal of ğšº, containing the singular values, has k non-zero elements.

To see the effect on the covariance matrix, we can fill in the SVD. You may have seen this before: filling in the SVD of ğ€ in the Gram matrix, and simplifying, gives us the eigendecomposition of the Gram matrix.

ğ€ğ€T\=ğ”ğšºğ•T(ğ”ğšºğ•T)T\=ğ”ğšºğ•Tğ•ğšºTğ”T\=ğ”ğšºğšºTğ”T\=ğ”ğšº2ğ”T.

Note that the square of a diagonal matrix like ğšº just consists of a diagonal matrix with the squares of the original matrix on the diagonal. That means that ğšº2 also has k non-zero values.

What does this tell us? Since this last line is the eigendecomposition, the diagonal values of ğšº2 are the [eigenvalues](https://gestalt.ink/eigenvalues) of the covariance matrix. They tell us how much the matrix ğ€ğ€T stretches space along the eigenvectors. If any of the eigenvalues are zero, as they are here, then along those directions, ğ€ğ€T _collapses_ space. By multiplying with zero, a whole dimension is collapsed into a single point. The result is that ğ€ğ€T is _singular_â€”the opposite of invertible.

So with that slight detour into singular value decompositions, we can characterize the covariance matrices of degenerate Gaussians. A covariance matrix is singular [if and only if](https://gestalt.ink/iff) the Gaussian is degenerate. If the Gaussian has full support, its covariance matrix is invertible.

Fundamental properties of Gaussians
-----------------------------------

Now that we have built up a geometric view of Gaussians, we can work out pretty much any property we need. Letâ€™s look at some examples. First, we know that linear transformations turn the standard Gaussian into another Gaussian. What happens if we linearly transform other Gaussians?

### Linear transformations

**Linear transformation of Gaussians** Let ğ± be a random variable with any Gaussian distribution ğ±âˆ¼N(Î¼,ğšº). Apply to ğ± any linear operation ğ²\=ğ€ğ±+ğ­ with a matrix ğ€ and vector ğ­. Then ğ² has a Gaussian distribution. Specifically,

ğ²âˆ¼N(ğ€Î¼+ğ­,ğ€ğšºğ€T).

Proof. We know, from our construction of the Gaussians, that there is some ğ and ğª so that ğ±\=ğğ¬+ğª with ğ¬\=N(ğŸ,ğˆ) gives us ğ±âˆ¼N(Î¼\=ğª,ğšº\=ğğT). Filling in this operation into the one from the theorem, we get

ğ²\=ğ€(ğğ¬+ğª)+ğ­\=ğ€ğğ¬+ğ€ğª+ğ­.

This expresses ğ² as a linear transformation of ğ¬âˆ¼N(ğŸ,ğˆ) with transformation matrix ğ€ğ and translation vector ğ€ğª+ğ­, so ğ² has a Gaussian distribution. Moreover, we know that its parameters are

Î¼ğ²\=ğ€ğª+ğ­\=ğ€Î¼+ğ­

and

ğšºğ²\=ğ€ğ(ğ€ğ)T\=ğ€ğğTğ€T\=ğ€ğšºğ€T.

Note, again, that this result holds, **even if ğ€ is not a square matrix**. This leads directly to some very useful corollaries.

**Subvectors of a Gaussian vector are Gaussian.** If we sample ğ± from any Gaussian, and select one or more of its elements, the resulting vector ğ±â€² is also distributed according to a Gaussian.

Proof. Selecting elements of a vector can be done by a matrix multiplication. For instance, the matrix (0,1,0) selects the middle element of a three-dimensional vector.

**question:** What does this look like if I select two elements? What are the parameters of the resulting distribution. What should I expect the resulting distribution to be if I select _all elements_? Can you show that this expectation is correct?

One consequence is that if you project a Gaussian onto one of the axes, the result is a univariate Gaussian along that axis. In terms of probability, this corresponds to _taking a marginal_. For example, if I measure the height and weight in a population of female soldiers, I get a bivariate distribution which is highly correlated (you can predict one measurement from the other pretty well) as we saw above. The above result shows that if I know the combined measurement is Gaussian, then dropping one of the two dimensions automatically results in a Gaussian as well.

With the example in the proof, we sample ğ± from some Gaussian, and then only look at the distribution on x2, disregarding the rest of the vector. If you followed the definition of marginalization, you would end up with a formula like

p(x2)\=âˆ«x1,x3N(x1,x2,x3âˆ£Î¼,ğšº)dx1x2

for which you would then have to fill in that horrible formula for N and work out the integral. Ultimately, you would end up with the result that p(x2) is a Gaussian, with some particular parameters, but it would be a lot of work.

This shows the benefit of our geometric construction of the Gaussians. With a little thinking we can almost always leave N be and never open up the box. We just assume that itâ€™s some affine transformation of the standard Gaussian and build up from there.

### If you can linearly transform it to a Gaussian, itâ€™s a Gaussian

We showed above that if you linearly transform a Gaussian, the result is another Gaussian. Next, itâ€™s useful to show that, under some mild assumptions, this also works the other way around. If we are given a distribution p and we can apply a linear transformation to turn it into a Gaussian, then p is also a Gaussian.

**Linear transformation _to_ Gaussians** Let ğ±âˆ¼p. If there exists a linear transformation ğ²\=ğ€ğ±+ğ­ so that ğ² is a Gaussian, then so long as the columns of ğ€ are linearly independent, p is Gaussian.

Proof. Since ğ² is Gaussian, there is a transformation ğ²\=ğğ¬+ğª with ğ¬âˆ¼N(ğŸ,ğˆ). This gives us

ğ€ğ±+ğ­ğ€ğ±ğ€Tğ€ğ±\=ğğ¬+ğª\=ğğ¬+ğªâˆ’ğ­\=ğ€Tğğ¬+ğ€T(ğªâˆ’ğ­)

The matrix ğ€Tğ€ on the left is called the [Gram matrix](https://gestalt.ink/gram-matrix) of ğ€. If ğ€â€™s columns are linearly independent, then the Gram matrix is invertible. This means we can multiply both sides by the inverse of the Gram matrix and get

ğ±\=ğ€â€ ğğ¬+ğ€â€ (ğªâˆ’ğ­)Â withÂ ğ€â€ \=(ğ€Tğ€)âˆ’1ğ€.

The right-hand-side, while complicated, is an affine transformation of a standard normally distributed vector ğ¬, so ğ± is Gaussian.

### There is always an _invertible_ transformation

We have defined a Gaussian as a distribution resulting from _any_ affine transformation ğ±\=ğ€ğ¬+ğ­ of standard-normal noise ğ¬. Even if a ğ€ is low-rank, so that the resulting Gaussian only covers a subspace of the space that ğ± is embedded in.

Letâ€™s focus on those Gaussians that are not degenerate in this way: assume, for an n\-dimensional vector ğ±, that the Gaussian ğ±\=ğ€ğ¬+ğ­ assigns every point in â„n a non-zero probability density.

ğ¬ might still be of a higher dimensionality than ğ±, and ğ€ may thus still not be invertible. In such a case, we can always find a different parametrization of the same Gaussian using an invertible matrix and an ğ¬ with the same dimension as ğ±.

**Invertible parametrization** Let ğ±\=ğ€ğ¬+ğ­ be any non-degenerate Gaussian. Then, there are parameters ğ, ğ® such that ğ is square and invertible and ğ±\=ğğ¬â€²+ğ® with ğ¬â€²âˆ¼N(ğŸ,ğˆ) describes the same Gaussian.

Proof. Let k be the dimensionality of ğ±. If ğ€ is taller than wide, it can not provide full support, so we may dismiss this case. If ğ€ is square and provides full support, then it must be invertible, so we can set ğ€\=ğ.

We are left with the case that ğ€ is rectangular and wider than tall. Let ğ€\=ğ”ğšºğ•T be the full singular value decomposition of ğ€. This gives us

ğ±\=ğ”ğšºğ•Tğ¬+ğ­.

We can rename ğ¬â€²\=ğ•Tğ¬. Since ğ• is an orthogonal transformation (a combination of rotations and flips), ğ¬â€² is still standard-normally distributed.

This gives us

ğ±\=ğ”ğšºğ¬â€²+ğ­.

![Image 11](assets/d/6/d6fc636efd0ce5cdecf6b50f459f536e.svg)

(left) A multiplication diagram for the operation above (with the +ğ­ term omitted). (right) The reduced version we will derive below.

From the multiplication diagram, we see that ğšº contains a number of zero columns which essentially ignore the corrsponding dimensions of ğ¬â€². Call ğ¬â€²k the vector ğ¬â€² with these dimensions removed, and call ğšºk the matrix ğšº with the corresponding columns removed. This gives us

ğ±\=ğ”ğšºkğ¬â€²k+ğ­.

We set ğ\=ğ”ğšºk to obtain the required result. Note that the diagonal of ğšº must contain all non-zero elements, or we would not have full support, so that it must be invertible. ğ” is also invertible, since it is orthogonal, and multiplying two invertible matrices together results in another invertible matrix.

If all that seems a bit technical, the key idea is that an affine transformation that results in full support on â„k must map k dimensions in the input to k dimensions in the output. The rest are dimensions that are ignored (they are in the _null space_ of ğ€). The trick is then to isolate just those k dimensions, and to ignore the rest.

The singular value decomposition is just a handy tool to isolate the right dimensions.

### The sum of two Gaussians is a Gaussian

Let ğš be a vector sampled from one Gaussian and ğ› be a vector sampled from another Gaussian. Sum them together and return the result ğœ\=ğš+ğ›. What is the distribution on ğœ?

It may not surprise you to learn that the result is another Gaussian.

It pays to be careful here. If I give you the probability density functions of two Gaussians, and you create a new probability density function by making a weighted sum of these two densities for a given value ğ±, then the result of that is a mixture-of-Gaussians, which is usually, decidedly _not_ Gaussian. What we are talking about here is _sampling_ from two different Gaussians, and then summing the sampled values.

**question**: I am a teacher and my class has students from two different schools in equal proportion, with different mean grades. The probability over the whole class of someone scoring a grade of 6 is the average of the probability that someone from school 1 scores a 6 and the probability that someone from school 2 scores a 6. Is the result necessarily a Gaussian? Consider what the distribution looks like if the mean grades for the two schools are very far apart.

**question**: I pair up each student from school 1 with a student from school 2. For one such pair, I test both, and average their grades. What is the distribution on the average I get? Is it Gaussian?

We can prove this property using our geometric construction, but we have to be a little bit more inventive than before. The key is to realize that the _concatenation_ of ğš and ğ› has a Gaussian distribution and that given this concatenation, the sum is just an affine operation.

Weâ€™ll first show that the concatenation of two Gaussians yields a Gaussian. This is a very intuitive result, that you may well be willing to accept without proof, but it doesnâ€™t hurt to be rigorous.

**Lemma. Concatenation of Gaussian variables** Let ğš and ğ› be vectors

ğšğ›ğœâˆ¼N(Î¼,ğšº),Â âˆ¼N(Î½,ğ“)Â andÂ \=(ğšğ›).

That is, ğœ is the concatenation of ğš and ğ›. Then p(ğœ) is Gaussian with mean (Î¼Î½) and covariance (ğšºğŸğŸğ“).

Proof. First, we rewrite ğš and ğ› as affine transformations of standard normal noise:

ğšğ›\=ğ€ğ¬+Î¼\=ğğ­+Î½.

Where ğ¬ and ğ­ are standard normal and ğšº\=ğ€ğ€T and ğ“\=ğğT. Then, ğœ can be written as

ğœ\=(ğšğ›)\=(ğ€ğ¬+Î¼ğğ­+Î½)\=ğ‚(ğ¬ğ­)+(Î¼Î½)

where

ğ‚\=(ğ€ğŸğŸğ).

Now, note that the vector (ğ¬ğ­) consists only of univariate, standard-normal elements. In other words, this vector is a standard-normal sample itself. This means that ğœ has a Gaussian distribution. From the affine transformation above, we see that its mean is the concatenation of Î¼ and Î½ as required. Its covariance is ğ‚ğ‚T, which the following diagram shows is equal to the covariance in the proof statement.

![Image 12](assets/7/8/78d4991756f98da3cbef030b73257e57.svg)

Using this lemma, the result for summing Gaussians follows almost directly.

**Theorem. Sum of Gaussian variables** Let

ğšğ›ğœâˆ¼N(Î¼,ğšº),Â âˆ¼N(Î½,ğ“)Â andÂ \=ğš+ğ›.

Then p(ğœ)\=N(ğœâˆ£Î¼+Î½,ğšº+ğ“).

Proof. Let k be the dimensionality of ğš and ğ›.

Let ğ be the concatenation of ğš and ğ›. By the lemma above, ğ follows a Gaussian distribution.

To turn two concatenated vectors into the sum of two vectors, we can multiply by the matrix (ğˆğˆ)â€”that is, two identity matrices side by side. If we have ğœ\=(ğˆğˆ)ğ, then

ci\=âˆ‘k(ğˆğˆ)ikdi\=ai+bi.

This shows that ğ is Gaussian.

To work out the parameters, we write out the full operation: concatenation and summation:

ğœ\=(ğˆğˆ)(ğ€ğŸğŸğ)(ğ¬ğ­)+(ğˆğˆ)(Î¼Î½)\=(ğ€ğ)ğ¬â€²+Î¼+Î½.

Which tells us that the mean is Î¼+Î½ and the covariance matrix is (ğ€ğ)(ğ€ğ)T\=ğ€ğ€T+ğğT.

### Chaining Gaussians

Hereâ€™s a situation that comes up occasionally. We sample a vector ğš from one Gaussian, and then make this the mean of another Gaussian. We then sample ğ› from the second Gaussian. Whatâ€™s the distribution on ğ›? If we are given the values of ğš, itâ€™s a Gaussian, thatâ€™s how we defined it. But what about p(ğ›). That is, what if someone told us only they had followed this process, but they didnâ€™t tell us what the value of ğš was? What probabilities would we assign to a given value of ğ›?

An example is trying to saw one plank to the length of another. You measure one plank and then saw the other to the length of your measurement. Both steps have some error: there is some error in how accurately you measure, and some error in how accurately you saw. Both processes are probably Gaussian: if you repeat the measurement or the sawing and plot the results, a bell shape will appear.

![Image 13](assets/d/b/db474abcd3c8558605db3141c6bf32d0.svg)

We measure plank 1, and then saw plank 2 to the measured length. Both actions have some inaccuracy in the form of Gaussian noise. Can we show that the distribution on the end result (a) is Gaussian and (b) has the length of the first plank as its mean?

The question now is what distribution we get if we donâ€™t know the measurement. Or, if you like, if we repeat the whole experiment many times. What will the distribution be on the length of plank we saw, combining both the uncertainty in the measuring and in the cutting.

It turns out that this distribution is Gaussian as well. One way to think of this distribution is as a _convolution_ of the two Gaussians we used for sampling. At every point ğ± in space we place a Gaussian. The probability density is a mixture of all these Gaussians, weighted by how likely we are to put a Gaussian at ğ±. Put differently, the probability p(ğ²) assigned to some point is a weighted â€œsumâ€â€”or more precisely an integralâ€”of all the Gaussians we could sample in the first step, all weighted by how likely they are to be sampled.

![Image 14](assets/8/e/8e6b4f77be8aee37e8e14850665d94e8.svg)

Imagine placing copies of one Gaussian along the number line at various points. We then take a weighted sum of these Gaussians, where the weight is the density of the point according to a second Gaussian. The result, as the number of points goes to infinity, is the _convolution_ of the two Gaussians.

We could use this integral to work out the shape of p(ğ²), but that would require lots of calculus. Instead, we will use our geometric perspective to take a shortcut.

**Theorem. Gaussian convolution.** Let

ğšğ›âˆ¼N(Î¼,ğšº)Â andÂ âˆ¼N(ğš,ğ“).

Then,

p(ğ›)\=N(Î¼,ğšº+ğ“).

Proof. From our geometric definition, we can rewrite ğš as ğš\=ğ€ğ¬+Î¼ with ğ¬ a standard-normally distributed vector, and ğšº\=ğ€ğ€T. Likewise we can write, ğ›\=ğğ­+ğš with ğ­ a _separate_ standard normally distributed vector and ğ“\=ğğT. Note that ğš takes the roles of the translation vector in the definition of ğ›.

In this view, we sample ğ¬ and ğ­, and then compute ğš and ğ› from them as regular vectors. That means we can plug the definition of ğš into that of ğ› and get

ğ›\=ğğ­+ğ€ğ¬+Î¼.

The first two terms, ğğ­+ğ€ğ¬ form the sum of two zero centered Gaussians. By the result of the previous section, this is equal to a single Gaussian with covariance ğšº+ğ“.

In the geometric view, we can say that ğ›\=ğ˜ğ®+Î¼, with ğ® standard normally distributed and ğ˜ğ˜T\=ğ€ğ€T+ğğT

Weâ€™ll work out the spherical case specifically as a corollary, since itâ€™s so central to diffusion models.

**Corollary. Spherical Gaussian convolution** Define ğš and ğ› as before, but with the constraint that they are spherical Gaussians with scalar standard deviations Ïƒ and Ï„ respectively. Then

ğ›ğ›âˆ¼N(Î¼,Ïƒ2+Ï„2)and\=Ïƒ2+Ï„2âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆšğ¬+Î¼

Proof. Take the result from the proof and let ğšº be diagonal matrix with every Î£ii\=Ïƒ and likewise for ğ“. Then ğšºğšºT is a diagonal matrix with all diagonal elements equal to Ïƒ2 and likewise for ğ“ğ“T so that

N(Î¼,ğšºğšºT+ğ“ğ“T)\=N(Î¼,Ïƒ2+Ï„2).

For the geometric definition of ğ›, note that the transformation matrix ğ€ should have the property that ğšº\=ğ€ğ€T. Since ğšº is the diagonal matrix (Ïƒ2+Ï„2)ğˆ, we can derive ğ€ simply by taking the square root of these diagonal elements

ğ€\=Ïƒ2+Ï„2âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆšğˆ.

### Conditioning Gaussians

What if we want to _condition_ ğ± on one or more of its values? For instance, we are interested in the distribution p(ğ±âˆ£x2\=3) where ğ± is drawn from a Gaussian. We can show that the result is, again, a Gaussian.

For a real-world example, we can look at our population of female soldiers again. If the combination of their heights and the weights is normally distributed, then what happens if we slice out only those soldiers that are 192cm tall? Do we get a Gaussian distribution on the weights in this subpopulation?

This one is a little more complex to prove. We will start with a lemma showing a single, specific, case. If ğ± is drawn from the standard normal distribution N(ğŸ,ğˆ), and we condition on one of the elements having a particular value c, then the resulting distribution p(ğ±âˆ£xi\=c) is standard normal on the remaining elements of ğ±. This result will require us to open the box and to look at the formula for N(ğŸ,ğˆ), but as we saw earlier, this formula is relatively straightforward.

With that lemma in place, we can then show our main result: that for _any_ variable y with a Gaussian distribution, conditioning on one of the elements of ğ² results in another Gaussian. This, we can do entirely by the affine operation trick.

**Lemma. Gaussian conditioning** Let ğ±âˆ¼Nn(ğŸ,ğˆ). Then for any element xi, and value c,

p(ğ±âˆ£xi\=c)

is a standard Gaussian Nnâˆ’1(ğŸ,ğˆ) on the remaining elements of ğ±.

Proof. To start with, consider how this conditional distribution is defined. In two dimensions, the situation looks like this.

![Image 15](assets/e/e/eea97175854e7339786572805fb4d9cf.svg)

The constraint xi\=c tells us that we assume that ğ± is on the red line. The probability density for points that are not on the line becomes zero. The density for points on the line stays the same, but should be rescaled uniformly so that the probability density, if we integrate over the whole line becomes 1.

Extending this to n dimensions, if we condition on one element xi of ğ±, the result is that the line becomes an nâˆ’1 dimensional hyperplane orthogonal to the i\-th axis. For any point in this hyperplane, we take the probability density under Nn(ğŸ,ğˆ) and rescale it, so that the whole hyperplane integrates to 1.

This integral sounds like a tricky one to work out. Luckily, we donâ€™t have to. We just assume it exists, and work around it with the â€œproportional toâ€ trick we saw earlier.

To make the notation simpler, we will assume, [without loss of generality](https://gestalt.ink/wlog), that xi is the last element of ğ±, that is xn. We call the vector ğ± with the n\-th element removed ğ±âˆ–n

Then if ğ± has xn\=c, we have

p(ğ±âˆ£xn\=c)âˆNn(ğŸ,ğˆ)\=expâˆ’12â€–ğ±â€–2\=expâˆ’12(x12+â€¦+xnâˆ’12+xn)\=expâˆ’12(x12+â€¦+xnâˆ’12+c)\=expâˆ’12(x12+â€¦+xnâˆ’12)â‹…expâˆ’12câˆexpâˆ’12(x12+â€¦+xnâˆ’12)\=expâˆ’12â€–ğ±âˆ–nâ€–2\=Nnâˆ’1(ğ±âˆ–nâˆ£ğŸ,ğˆ).

We see that the probability density that p(ğ±âˆ£xn\=c) assigns to the vector ğ±, if xn\=c, is proportional to the density that Nnâˆ’1(ğ±âˆ–n) assigns to the first nâˆ’1 elements of ğ±. Normally, to turn this into a fully determined probability function, we need to figure out what this integrates to and divide by that to turn the âˆ into a \=. However, in this case, we know what the right-hand side integrates to, because Nnâˆ’1 is already a proper probability density function, and we are allowing all possible values for ğ±âˆ–n. It integrates to 1, so we can simply say that

p(ğ±âˆ£xn\=c)\=Nnâˆ’1(ğŸ,ğˆ).

Why doesnâ€™t this argument hold for Gaussians in general? Itâ€™s the â€œorthogonalâ€ structure of the standard Gaussian. This allows us to remove one dimension, after which we are left simply with a standard Gaussian of one dimension fewer.

However, we can build on this result to show that conditioning in general produces a Gaussian. All we need to do is to show that a conditioned Gaussian can be transformed _to_ a conditioned standard normal Gaussian. Since anything that can be transformed to a Gaussian is itself Gaussian (so long as the transformation has linearly independent columns), this proves the result.

**Theorem. Gaussian conditioning.** If we sample ğ± from any Gaussian with full support, and condition on one of its elements, the resulting distribution, p(ğ±âˆ£xi\=c), is Gaussian.

Proof.

![Image 16](assets/1/f/1fdf8c33199d7e4c94855267626b0513.svg)

The key idea of the proof. The ğ¬â€™s that result in samples ğ± that satisfy xi\=c form a hyperplane constraint on the standard Gaussian on ğ¬. With a simple rotation, which doesnâ€™t affect the density, we translate to the situation of the lemma. We can now say that we can transform _from_ p(ğ±âˆ£xi\=c) to p(ğ¬âˆ£sj\=câ€²) by an invertible, affine operation. As we showed earlier, this means that p(ğ±âˆ£xi\=c) must be Gaussian.

Since p(ğ±) is Gaussian, there is some invertible ğ€ and ğ­ so that ğ±\=ğ€ğ¬+ğ­ with ğ¬âˆ¼N(ğŸ,ğˆ). This means that xi\=ğšiğ¬+ti, where ğši is the i\-th row of ğ€.

Our conditioning xi\=c, gives us ğšiğ¬+ti\=c, a linear constraint on the values of ğ¬. Since itâ€™s an extra constraint in one variable, it essentially means that if we know all values of ğ¬ except one, say s1, then we can work out what s1 must be. We can show this with some simple re-arranging:

yi\=cs1\=Ai1s1+â€¦+Ainsn+ti\=âˆ’1Ai1(Ai2s2+â€¦+Ainsn+tiâˆ’c).

The last line represents a constraint on ğ¬. Weâ€™ll refer to this constraint as c(ğ¬), a boolean function which is true if the constraint holds for ğ¬.

Now, since c(ğ¬) linearly expresses one element of ğ¬ in terms of the other nâˆ’1, the ğ¬â€™s that satisfy it form an nâˆ’1 dimensional hyperplane. Itâ€™s not axis-aligned, as it was in the lemma before, but that can be fixed with a simple rotation. Let ğ‘ be an orthogonal matrix such that the transformation

ğ³\=ğ‘ğ¬

when applied to the hyperplane c(ğ¬) yields a hyperplane orthogonal to the n\-th axis.

Since N(ğŸ,I) is rotationally symmetric, the density of any point ğ¬ remains unaffected when it is mapped to ğ³. This tells us that p(ğ¬âˆ£c(ğ¬))\=p(ğ³âˆ£zn\=câ€²) for some value câ€².

And with that, we can apply our lemma. p(ğ³âˆ£zn\=câ€²) is a standard Gaussian, by the lemma. p(ğ±âˆ£c(ğ±)) is an orthogonal transformation of it, so also a standard Gaussian, and p(ğ±âˆ£xk\=c) is an affine transformation of that (with an invertible matrix), so also Gaussian.

Finally, if we want to condition on more than one element of ğ±, we could repeat the same proof structure any dimension of hyperplane, but itâ€™s simpler to just apply the theorem multiple times.

**Corollary. Gaussian conditioning on multiple elements.** If we sample ğ² from any Gaussian, and condition on m of its elements, the resulting distribution is Gaussian.

Proof. Assume we have a Gaussian p(x1,â€¦,xn). Conditioning on x1 gives us, by the theorem, a Gaussian p(x2â€¦,xnâˆ£x1). Since the latter is a Gaussian, we can condition on one of its elements and, by the theorem get another Gaussian p(x3,â€¦xnâˆ£x1,x2). We can do this for any number of elements, and in any order we like.

**question:** What if you want to know not just whether the conditional a Gaussian is, but _which_ Gaussian? I.e. what are its parameters? How would you proceed? Which elements of the proof would you need to work out in greater detail?

Deriving the density
--------------------

To finish, we will see where that gargantuan formula comes from. With the picture we have built up, of Gaussians as affine transformations of a single standard Gaussian, itâ€™s not so complex to derive. We just need one small trick.

To build our intuition, letâ€™s look at a very simple 2D Gaussian, as defined by the transformation

ğ±\=ğˆğ¬+(11).

That is, we donâ€™t squish or stretch the standard-normal distribution, we just shift it around a bit, _translate_ it.

To figure out what the density of a particular point ğ± is, all we need to do is shift it back. For example, if ğ±\=(21), a point one unit above the mean, we can work out the density by shifting this point back to (10), the point one unit above the mean of the standard Gaussian. Since the two distributions are just translations of each other, these points will have the same density under their respective distributions.

![Image 17](assets/c/4/c41d5bf06b4768a69f6e1ce2edfb6275.svg)

This tells us that we can express the density of our new function in terms of the density function we already have for the standard Gaussian.

N(ğ±âˆ£ğˆ,ğ­)\=N(ğ¬âˆ’ğ­|ğˆ,ğŸ)

Now, we know the density function of the standard Gaussian, thatâ€™s

N(ğ¬âˆ£ğˆ,ğŸ)\=1zexpÂ âˆ’12â€–ğ¬â€–2.

So, if we fill in ğ¬\=ğ±âˆ’ğ­ we get

N(ğ±âˆ£ğˆ,ğ­)\=1zexpÂ âˆ’12â€–ğ±âˆ’ğ­â€–2.

The idea is simple: for a given Gaussian expressed as a transformation of the standard Gaussian, we transform ğ± back to the standard Gaussian, by inverting the transformation, and then we just read off the density.

Can we apply the same idea to the transformation matrix ğ€? Here we have to be a bit more careful. As we transform by ğ€, it may stretch or shrink space. Itâ€™s easiest to see what might go wrong in the 1D case:

![Image 18](assets/8/e/8e4dce2724da713084294581b4977c9b.svg)

As you can see, if we have Ïƒ\=1/2, then after we multiply by 1/Ïƒ\=2, the whole function blows up as a result. This means that the area under the curve will no longer sum to 1. Luckily, the increase is simply a factor of Ïƒ, so if we apply the change of variables naively, all we have to do is divide the result by 1/Ïƒ to correct the error.

If youâ€™re not convinced, imagine approximating the bell curve by a series of boxes. Multiplying by Ïƒ stretches each box horizontally but not vertically, so the area goes from height times width to height Ã— width Ã—Ïƒ. When we sum over all boxes, we can take Ïƒ out of the sum to see that the total area is multiplied once by Ïƒ. Now let the width of the boxes shrink to get a better and better approximation.

The same reasoning applies in higher dimensions. Assume we have our Gaussian described in terms of an invertible transformation matrix ğ€â€”which, as we showed before, is always possible.

We can carve up the plane of a 2D Gaussian into squares, and approximate the _volume_ under the surface with a series of rectangular columns on top of these.

The result of transforming the Gaussian by ğ€ is that the squares are stretched into parallelograms. Happily, because itâ€™s a linear transformation, every square is stretched into a parallelogram of the same size. We know much the surface area shrinks or increases, because thatâ€™s simply the determinant of ğ€.

With that, we can establish our formula for the density. We know that ğ±\=ğ€ğ¬+ğ­ defines our Gaussian. To work out the density of ğ±, all we need to do is invert our function to find the corresponding ğ¬, take its density, and then correct for the amount by which ğ€ inflates space by multiplying with 1/|ğ€|.

N(ğ±âˆ£ğ€,ğ­)\=N(ğ¬\=ğ€âˆ’1(ğ±âˆ’t)âˆ£ğˆ,ğŸ)\=1|ğ€|Ã—1zexpÂ âˆ’â€–ğ€âˆ’1(ğ±âˆ’t)â€–2\=1z|ğ€|expÂ âˆ’(ğ€âˆ’1(ğ±âˆ’t))T(ğ€âˆ’1(ğ±âˆ’t))\=1z|ğ€|expÂ âˆ’(ğ±âˆ’t)T(ğ€Tğ€)âˆ’1(ğ±âˆ’t)

In the last two lines, we've used the properties that â€–zâ€–2\=ğ³Tğ³, that ğ€âˆ’1ğâˆ’1\=(ğğ€)âˆ’1 and that (ğ€âˆ’1)T\=(ğ€T)âˆ’1.

This is the density function of a Gaussian expressed in the geometric parameters. If we want to translate this to the more common parametrization in terms of the covariance matrix ğšº\=ğ€Tğ€, we just need to note that the determinant has the properties that |ğ€ğ|\=|ğ€|Ã—|ğ| and |ğ€T|\=|ğ€|, so that

|ğ€|\=|ğ€Tğ€|12\=|ğšº|12.

Filling this in, we get

1z|ğšº|12expÂ âˆ’(ğ±âˆ’t)Tğšºâˆ’1(ğ±âˆ’t).

This is exactly the formula we started with at the top of the article, except that we havenâ€™t bothered to work out z\=âˆ«âˆâˆ’âˆexpÂ âˆ’12|ğ±|2. This is called the [Gaussian integral](https://gestalt.ink/gaussian-integral), and as you can tell from the completed formula, it works out to (2Ï€)âˆ’12d, where d is our dimension. We will leave this working out to another article.

You may ask what happens if ğ€ is not invertible. Well, then ğšº isnâ€™t either, and the traditional parametrization breaks down. However, in the geometric parametrization, we have some hope left. We know that we still have a Gaussian on our hands, with a bell shape and everything. Itâ€™s just that it only covers a linear subset of space.

Sources and other materials
---------------------------

*   We discuss these same topics in our [lecture on Diffusion models](https://dlvu.github.io/diffusion/) in the [deep learning course at the Vrije Universiteit Amsterdam](https://dlvu.github.io/). This lecture is a little more compact than this article, which may help you to get the general overview. It comes with videos (or will do soon).

References
----------

\[1\] Paquette, S. (2009). [Anthropometric survey (ANSUR) II pilot study](https://www.openlab.psu.edu/ansur2/): methods and summary statistics. Anthrotch, US Army Natick Soldier Research, Development and Engineering Center.
